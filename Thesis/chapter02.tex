This chapter reviews current research regarding online failure prediction and
its many approaches to build a foundation for this research.  Further, a
taxonomy of approaches was developed here~\cite{salfnerSurvey}, this chapter
updates that taxonomy and classifies approaches since its publication using it.

The rest of this chapter is organized as follows.  In Section~\ref{background},
a brief background on the topic of online failure prediction (OFP) is given
including definitions, terminology, and measures of performance used by the
community.  In Section~\ref{approaches}, the approaches relevant to this
research are presented followed by a brief summary in Section~\ref{summary}.

\section{Background} \label{background}
In 2010, Salfner, et al.~\cite{salfnerSurvey} published a survey paper that
provides a comprehensive summary of the state of the art on the topic of OFP.
In addition to the review of the literature up to the point of publication,
they provide a summary of definitions and measures of performance commonly used
in the community for couching the OFP discussion.

\subsection{Definitions} \label{definitions}
\subsubsection{Proactive Fault Management:} \label{pfm}
Salfner, et al.~\cite{salfnerSurvey} define proactive fault management (PFM) as
the process by which faults are handled in a proactive way, analogous with
\emph{fault tolerance} and basically consisting of four steps: online failure
prediction, diagnosis, action scheduling, and action execution as shown in
Figure~\ref{fig:proactiveFaultManagement}.
The final three stages of PFM define how much lead time is required to avoid a
failure when predicted during OFP.  \emph{Lead time} is defined as the time
between when failure is predicted and when that failure will occur.  Lead time
is one of the most critical elements of a failure prediction approach.

\figproactiveFaultManagement

OFP is defined as the first step in PFM shown in
Figure~\ref{fig:proactiveFaultManagement}.  OFP is the act of analyzing the
running state of a system in order to predict a failure in that system. Once
failure has been predicted, a fault tolerant system must determine what will
cause the failure.  This stage is called the \emph{diagnosis} stage or
``root-cause analysis'' stage.  During the \emph{diagnosis} stage, the analysis
must be conducted so that a system knows which remediation actions are
possible.  After it is determined what will cause a failure, a fault tolerant
system must schedule a remediation action that is either performed by an
operator or done automatically.  This stage is known as the \emph{action
scheduling} stage and normally takes as input the cost of performing an action,
confidence in prediction, effectiveness/complexity of remedy action and makes a
decision about what action to perform based on that input.  In some cases a
remedy action can be so simple that even if the confidence in the prediction is
low, the action can still be performed with little impact on the overall system
and its users.  A thorough analysis of the trade-off between cost of avoidance
and confidence in prediction and the associated benefits is described
in~\cite{candea2004microreboot}.  Finally, in order to avoid failure, a system
must execute the scheduled remediation action or let an operator know which
actions can be taken in a stage called \emph{action execution}.

\subsubsection{Faults, Errors, Symptoms, and Failures:}
This research uses the definitions from~\cite{avivzienis2004basic} as
interpreted and extended in~\cite{salfnerSurvey} for the following terms:
failure; error (detected versus undetected); fault; and symptom.

\emph{Failure} is an event that occurs when the delivered service deviates from
correct service.  In other words, things can go wrong internally; as long as
the output of a system is what is expected, failure has not occurred.  

An \emph{error} is the part of the total state of the system that may lead to
its subsequent service failure.  \emph{Errors} are characterized as the point
when things go wrong~\cite{salfnerSurvey}.  Fault tolerant systems can handle
errors without necessarily evolving into failure.  There are two kinds of
errors.  First, a \emph{detected error} is an error that is reported to a
logging service.  In other words, if it can be seen in a log then it is a
detected error.  Second, \emph{undetected errors} are errors that have not been
identified by an error detector.  Undetected errors are things like memory
leaks.  The error exists, but as long as there is usable memory, it is not
likely to be reported to a logging service.  Once the system runs out of usable
memory, undetected errors will likely appear in logs and become a detected
errors.  A \emph{fault} is the hypothesized root cause of an error.  Faults can
remain dormant for some time before manifesting themselves and causing an
incorrect system state.  In the memory leak example, the missing \emph{free}
statement in the source code would be the fault.  

A \emph{symptom} is an out-of-norm behavior of a system's parameters caused by
errors, whether detected or undetected.  In the memory leak example, a possible
symptom of the error might be delayed response times due to sluggish
performance of the overall system.

\figfailureFlowDiagram

Figure~\ref{fig:failureFlowDiagram} illustrates how a software fault can evolve
into a failure.  Faults, errors, symptoms, and failures can be further
categorized by how they are detected also shown in
Figure~\ref{fig:failureFlowDiagram}.  Salfner, et al.~\ref{salfnerSurvey}
introduces a taxonomy of OFP approaches and classifies failure prediction
approaches by the stage at which a fault is detected as it evolves into a
failure: auditing, reporting, monitoring, and tracking.  Testing is left out
because it does not help detect faults in an online sense.  

\figonlinePrediction

Figure~\ref{fig:onlinePrediction} demonstrates the timeline associated with
OFP.  The parameters used by the community to define a predictor are as
follows:
\begin{itemize}
	\item{Present Time: $t$}
  \item{Lead Time: $\Delta t_{l}$, is the total time at which a predictor makes
  an assessment about the current state.}
  \item{Data Window: $\Delta t_{d}$, represents the time from which data is
  used for a predictor uses to make its assessment.}
  \item{Minimal Warning Time: $\Delta t_{w}$, is the amount of time required to
  avoid a failure if one is predicted.}
  \item{Prediction Period: $\Delta t_{p}$, is the time for which a prediction
  is valid.  As $\Delta t_{p} \rightarrow \infty$, the accuracy of the
  predictor approaches 100\% because every system will eventually fail.  As
  this happens, the usefulness of a predictor is diminished.}
\end{itemize}

As the above parameters are adjusted, predictors can become more or less
useful.  For example, it is clear that as a predictor looks further into the
future potentially increasing \emph{lead time}, confidence in its prediction is
likely to be reduced.  On the other hand, if \emph{lead time} is too small,
there will likely not be enough time to effectively take remediation action.
In general, OFP approaches seek to find a balance between the parameters,
within an acceptable bound depending on application, to achieve the best
possible performance.

\subsection{Performance Measures} \label{metrics}
In order to accurately compare OFP approaches, standard performance measures 
are used.  A widely accepted set of performance measures used by the
community is presented in~\cite{salfnerSurvey}.  Before we begin the outline of
the performance measures used to evaluate and compare failure prediction
approaches, it is important to note that OFP is done based on statistical
analysis of known data.  In other words, in order to calculate the following
outlined performance measures, predictors must be evaluated against labeled
data sets.  Typically, the labeled data set is divided into three parts:
\begin{enumerate}
\item{Training Set:  A data set that allows a prediction model to establish and
optimize its parameters}
\item{Validation Set:  The parameters selected in the training phase are then
validated against a separate data set}
\item{Test Set:  The predictor is finally run against a final previously
unevaluated data set to assess generalizability}
\end{enumerate}
During the test phase, true positives (negatives) versus false positives
(negatives) are determined in order to compute the performance measures in
this section.  The following terms and associated abbreviations are used:
\emph{True Positive} (TP) is when failure has been predicted and then actually
occurs; \emph{False Positive} (FP) is when failure has been predicted and then
does not occur; \emph{True Negative} (TN) is when a state has been accurately
classified as non-failure prone; \emph{False Negative} is when a state has been
classified as non-failure prone and a failure occurs.

\subsubsection{Precision and Recall:}
Precision and recall are the most popular performance measures used when
for comparing OFP approaches.  The two are related and often times
improving precision results in reduced recall.  Precision is the number of
correctly identified failures over the number of all predicted failures.  In
other words, it reports, out of the predictions of a failure-prone state that
were made, how many were correct.  In general, the higher the precision the
better the predictor.  Precision is expressed as:

\[ Precision 
	= \dfrac{TP}{TP + FP} \in [0,1]
\]

Recall is the ratio of correctly predicted failures to the number of true
failures.  In other words, it reports, out of the actual failures that
occurred, how many the predictor classified as failure-prone.  In conjunction
with a higher precision, higher recall is indicative of a better predictor.
Recall is expressed as:

\[ Recall 
	= \dfrac{TP}{TP + FN} \in [0,1]
\]

F-Measure, as defined by~\cite{rijsbergen1979v}, is the harmonic mean of
precision and recall and represents a trade-off between the two.  A higher
F-Measure reflects a higher quality predictor.  F-Measure is expressed as:

\[ F\mhyphen Measure 
	= \dfrac{2 \cdot Precision \cdot Recall}{Precision + Recall} \in [0,1]
\]

\subsubsection{False Positive Rate and Specificity:}
Precision and recall do not account for true negatives (correctly predicted
non-failure-prone situations) which can bias an assessment of a predictor.  The
following performance measures take true negatives into account to help
evaluators more accurately assess and compare predictors.

False Positive Rate (FPR) is the number of incorrectly predicted failures over
the total number of predicted non-failure-prone states.  A smaller FPR reflects
a higher quality predictor.  The False Positive Rate is expressed as:

\[ \mathit{FPR}
	= \dfrac{FP}{FP + TN} \in [0,1]
\]

Specificity the number of times a predictor correctly classified a state as
non-failure-prone over all non-failure-prone predictions made.  In general,
specificity alone is not very useful since failure is rare.  Specificity is
expressed as:

\[ Specificity 
	= \dfrac{TN}{FP + TN} = 1 - False Positive Rate
\]

\subsubsection{Negative Predictive Value (NPV) and Accuracy:}
In some cases, we wish to show that a prediction approach can correctly
classify non-failure-prone situations.  The following performance measures 
usually can not stand alone due to the nature of failures being rare events.
In other words, a highly ``accurate'' predictor could classify a state 100\% of
the time as non-failure-prone and still fail to predict every single true
failure.

Negative Predictive Value (NPV) is the number of times a predictor correctly
classifies a state as non-failure-prone to the total number all
non-failure-prone states during which a prediction was made.  Higher quality
predictors have high NPVs.  The NPV is expressed as:

\[ \mathit{NPV}
	= \dfrac{TN}{TN + FN}
\]

Accuracy is the ratio of all correct predictions to the number of predictions
made.  Accuracy is expressed as:

\[ Accuracy 
	= \dfrac{TP + TN}{TP + FP + FN + TN}
\]

\subsubsection{Precision/Recall Curve:}
Much like with other predictors, many OFP approaches implement variable
thresholds to sacrifice precision for recall or vice versa.  That trade-off is
typically visualized using a precision/recall curve as shown in
Figure~\ref{fig:precisionRecallCurve}.

\figprecisionRecallCurve

Another popular visualization is the receiver operating characteristic (ROC)
curve.  By plotting true positive rate over false positive rate one is able to
see the predictors ability to accurately classify a failure.  A sample ROC
curve is shown in Figure~\ref{fig:ROC}.

\figROC

The ROC curve relationship can be further illustrated by calculating the area
under the curve (AUC).  Predictors are commonly compared using the AUC which is
calculated as follows:

$$AUC = \int_{0}^{1} \mathit{tpr}(\mathit{fpr}) \,d\,\mathit{fpr} \in [0,1],$$

\noindent
where $tpr$ = true positive rate (recall), and $fpr$ = false positive rate.  A
pure random predictor will result in an AUC of $0.5$ and a perfect predictor a
value of~$1$.  The AUC can be thought of as the probability that a predictor
will be able to accurately distinguish between a failure-prone state and a
non-failure-prone state, over the entire operating range of the predictor.

\section{Approaches to Online Failure Prediction (OFP)} \label{approaches}
\subsection{OFP Taxonomy}
The taxonomy by Salfner, et al.~\cite{salfnerSurvey} classifies many of the OFP
approaches in the literature into four major categories.  These four major
categories are defined by the four techniques used to detect faults in
real-time: auditing, monitoring, reporting, and tracking.

Since this research focusses on real-time \emph{data-driven} device failure
prediction approaches, our focus is on the \emph{reporting} category of
Salfner's taxonomy.  The \emph{reporting} category organizes failure prediction
techniques that attempt to classify a state as failure prone based on reported
errors.  Salfner, et al.~\cite{salfnerSurvey} further organize the reporting
category into five sub-categories: rule-based systems; co-occurrence; pattern
recognition; statistical tests; and classifiers.

\emph{Rule-Based Systems} attempt to classify a system as being failure-prone
or not based a set of conditions met by reported errors.  Since modern systems
are far too complex to build a set of conditions manually, these approaches
seek to find automated ways of identifying these conditions in training data.
\emph{Co-occurrence} predictors generate failure predictions based on the
reported errors that occur either spatially or temporally close together.
\emph{Pattern Recognition} predictors attempt to classify patterns of reported
errors as failure prone.  This research focusses on pattern recognition OFP
approaches, which can be visualized in Figure~\ref{fig:patternRecognition}.
\emph{Statistical Tests} attempt to classify a system as failure-prone based on
statistical analysis of historical data.  For example, if a system is
generating a much larger volume of error reports than it typically does, it may
be a sign of pending failure.  \emph{Classifiers} assign labels to given sets
of error reports in training data and then make failure predictions based on
observed labels in real-time data.

\figpatternRecognition

\subsection{Data-Driven Online Failure Prediction}
% The survey published by Salfner et al. covered approaches in every sub-category
% of the \emph{reporting} category.  Since the publication of the survey, we
% found approaches in two of the subcategories, \emph{pattern recognition} and
% \emph{classifiers}.  We therefore only cover the approaches in those
% sub-categories of the reporting category here.  We found some of the approaches
% published since Salfner's survey to be difficult to classify because they
% employ aspects of the other sub-categories in the \emph{reporting} category.
% More specifically, many of the modern techniques seem to be a blend between the
% two sub-categories \emph{pattern recognition} and \emph{classifiers}.  We
% believe these categories have been blended because these approaches seem to
% follow general human intuition when looking for software failures.  In other
% words, we have found that cyber operators tend to look for patterns in reported
% errors and then classify a situation based on those patterns.  We therefore
% categorize these approaches as \emph{hybrid} approaches.

\subsubsection{Pattern Recognition:}
Salfner, et al.~\cite{salfner2006} proposed an approach to predicting failures
by learning patterns of similar events using a semi-Markov chain model.
The model learned patterns of error reports that led to failure by mapping the
reported errors to the states in the Markov chain and predicted the probability
of the transition to a failure-prone state.  They tested the model using
performance failures of a telecommunication system and reported a precision of
0.8, recall of 0.923, and an F-measure of 0.8571, which drastically
outperformed the models to which it was compared.

Given the results, the semi-Markov Chain model is compelling however, it
depends on the sequence of reported errors to remain constant in order to be
effective.  Today, most software is multi-threaded or distributed so there is
no guarantee that the sequence of reported errors will remain constant.
Further, the authors reported that this approach did not scale well as the
complexity of the reported errors grew.

In 2007, Salfner et al. extended their previous work in~\cite{salfner2006}
using semi-Markov models~\cite{salfner2007}.  They generalized the Hidden
Semi-Markov process for a continuous-time model and called it the Generalized
Hidden Semi-Markov Model (GHSMM).  By making this generalization, the model
was able to effectively predict the sequence of similar events (or in this
case, errors) in the continuous time domain.  The authors then tested the model
and training algorithm using telecommunication performance failure data and
compared it to three other approaches.  While this GHSMM model did not perform
as well as their previous work, it did outperform the models to which it was
compared and more importantly did not depend on the sequence of reported
errors.  In other words, this new GHSMM model predicted failure for
permutations of a known failure-prone sequence making it more suited for a
distributed or parallel system.

The GHSMM approach has been well received by the community, although appears to
be limited in use to a single system.  Unfortunately, this approach as well as
its predecessor, does not scale well and does not adapt to changes to the
underlying system without retraining.

\subsubsection{Classifiers:}
Domeniconi, et al.~\cite{domeniconi2002} published a technique based on
support vector machines (SVM) to classify the present state as either failure
prone or not based on a window of error reports as an input vector.  As Salfner
points out in~\cite{salfnerSurvey}, this SVM approach would not be useful
without some sort of transformation of the input vector since the exact same
sequence of error messages, rotated by one message, would not be classified as
similar.  To solve this permutation challenge, the authors
in~\cite{domeniconi2002} used singular value decomposition to isolate the
sequence of error reports that led to a failure.

This SVM approach used training data from a production computer environment
with 750 hosts over a period of 30 days.  The types of failures the system was
trying to detect was the inability to route to a web-page and an arbitrary node
being down.  Many approaches involving SVMs have been explored since and seem
to be popular in the community~\cite{fronza2013, fulp2008, murray2005,
domeniconi2002, irrera2015}.

\subsubsection{Hybrid Approaches:}
\emph{Fujitsu Labs} has published several papers on an approach for predicting
failure in a cloud-computing
environment~\cite{sonoda2012,watanabe2012,watanabe2014}.  Watanabe, et
al.~\cite{watanabe2014, watanabe2012} report on findings after applying a
Bayesian learning approach to detect patterns in similar log messages.  Their
approach abstracts the log messages by breaking them down into single words and
categorizing them based on the number of identical words between multiple
messages.  This hybrid approach removes the details from the messages, like
node identifier, and IP address while retaining meaning of the log message.

Watanabe et al.'s~\cite{watanabe2014} hybrid approach attempts to solve the
problem of underlying system changes by learning new patterns of messages in
real-time.  As new messages come in, the model actively updates the probability
of failure by Bayesian inference based on the number of messages of a certain
type that have occurred within a certain time window.  The authors claim that
their approach solves three problems: 1)  The model is not dependent upon a
certain lexicon used to report errors to handle different messages from
different vendors; 2) The model does not take into account the order of
messages necessarily so in a cloud environment where messages may arrive in
different orders, the model is still effective; and 3)  The model actively
retrains itself so manual re-training does not need to occur after system
updates.  The model was then tested in a cloud environment over a ninety day
period.  The authors reported a precision of 0.8 and a recall of 0.9, resulting
in an F-measure of 0.847.  

Fronza, et al.~\cite{fronza2013} introduced a pattern-recognition/classifier
hybrid approach that used an SVM to detect patterns in log messages that would
lead to failure.  The authors used random indexing to solve the problem
previously discussed of SVMs failing to classify two sequences as similar if
they are offset by one error report.  The authors report that their predictor
was able to almost perfectly detect non-failure conditions but was poor at
identifying failures.  The authors then weighted the SVMs to account for this
discrepancy by assigning a larger penalty for false negatives than false
positives and had better results.

\subsection{Industry Approaches to Online Failure Prediction} \label{industry}
Because hardware has become so easy to acquire, industry has sought to avoid
the problem of software failure by implementing massive redundancy in their
systems.  The work in~\cite{watanabe2014,irrera2015} attributes the problem
avoidance to the fact that until recently, implementing and maintaining a
failure predictor was difficult.  As we decrease the length of the software
development life cycle, software updates are being published with increasing
frequency leading to rapid changes in underlying systems.  These changes can
often render a predictor useless without re-training, which is often a manual
and resource intensive process.

Redundancy is not without problems however.  Implementing redundant systems to
avoid the failure problem can be expensive and can add overhead and complexity
making a system more difficult to manage.

\subsection{Adaptive Failure Prediction (AFP) Framework} \label{afp}
The Adaptive Failure Prediction (AFP) Framework by Irrera, et al.
in~\cite{irrera2015} seen in Figure~\ref{fig:AFP} presents a new approach to
maintaining the efficacy of failure predictors given underlying system changes.
The authors conducted a case study implementing the framework using
virtualization and fault injection on a web server.  

\figAFP

The concept reported used past work by Irrera et
al.~\cite{irrera2013,irrera2014} to generate failure data by injecting software
faults using a tool based on G-SWFIT~\cite{gswfit} in a virtual environment for
comparing and automatically re-training predictors.  In general, the use of
simulated data is not well received by the community, however the authors
in~\cite{irrera2010,irrera2014} report evidence supporting the claim that
simulated failure data is representative of real failure data.  Further, the
authors suggest that since systems are so frequently updated and failures are
in general rare events, real failure data is often not available.  Moreover,
the literature shows that even if there is a certain type of failure in
training data and a predictor can detect and predict that type of error
accurately, it will still miss failures not present in the training data.  By
injecting the types of faults that one can expect, each failure type is
represented in the training data.

The authors then conducted a case-study using a web server and an SVM
predictor, and report their findings demonstrate their framework is able to
adapt to changes to an underlying system which would normally render a
predictor unusable.  They reported good results and concluded that the AFP is
an effective tool.  Unfortunately, the AFP is not a universal solution and
requires significant work to be implemented on a modern Microsoft Windows
enterprise network.  Furthermore, the fault load previously explored does not
completely represent all possible failures.

\section{Summary} \label{litReviewSummary}
This chapter covered the definitions, measures of performance, and approaches
that are relevant to this research as organized under the subsection of
\emph{reporting} within the OFP field of study.  There has been a tremendous
amount of research surrounding the topic of OFP and many prediction approaches
have been presented.  Unfortunately, these approaches do not appear on modern
operational systems and failures are still relatively prevalent.  Recent
approaches as covered here have sought to make predictors more adaptive to the
changes in underlying systems in an effort to make implementing existing
failure predictors easier.  In this work, we plan to extend the adaptive
failure prediction framework and further generalize the approach.  
