\subsection{OFP Taxonomy}
The taxonomy by Salfner, et al.~\cite{salfnerSurvey} classifies many of the OFP
approaches in the literature into four major categories.  These four major
categories are defined by the four techniques used to detect faults in
real-time: auditing, monitoring, reporting, and tracking.

Since this research focusses on real-time \emph{data-driven} device failure
prediction approaches, our focus is on the \emph{reporting} category of
Salfner's taxonomy.  The \emph{reporting} category organizes failure prediction
techniques that attempt to classify a state as failure prone based on reported
errors.  Salfner, et al.~\cite{salfnerSurvey} further organize the reporting
category into five sub-categories: rule-based systems; co-occurrence; pattern
recognition; statistical tests; and classifiers.

\emph{Rule-Based Systems} attempt to classify a system as being failure-prone
or not based a set of conditions met by reported errors.  Since modern systems
are far too complex to build a set of conditions manually, these approaches
seek to find automated ways of identifying these conditions in training data.
\emph{Co-occurrence} predictors generate failure predictions based on the
reported errors that occur either spatially or temporally close together.
\emph{Pattern Recognition} predictors attempt to classify patterns of reported
errors as failure prone.  This research focusses on pattern recognition OFP
approaches, which can be visualized in Figure~\ref{fig:patternRecognition}.
\emph{Statistical Tests} attempt to classify a system as failure-prone based on
statistical analysis of historical data.  For example, if a system is
generating a much larger volume of error reports than it typically does, it may
be a sign of pending failure.  \emph{Classifiers} assign labels to given sets
of error reports in training data and then make failure predictions based on
observed labels in real-time data.

\figpatternRecognition
