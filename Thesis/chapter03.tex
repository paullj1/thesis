The purpose of the AFP framework is to automate the generation of realistic
labelled failure data for the purposes of automatically training a failure
prediction algorithm.  The framework breaks down into modules so that the it
can be more easily adapted for different applications.  This chapter presents
three topics.  The first describes the process that the extended framework
executes in order to generate the labelled training data and automatically
train a failure prediction algorithm.  The second outlines each module of the
extended AFP framework and their associated extensions in detail.  The final
section outlines extensions to the AFP not covered in the other two sections.

This chapter outlines the implementation and extensions to the Adaptive Failure
Prediction (AFP) Framework as seen in~\cite{irrera2015} as well as an
experiment to validate those extensions and further generalize the framework.
The AFP was originally tested on a single system running an operating system
that has been deprecated.  Consequently, the results from the case study
conducted using the AFP are limited in utility and require generalization to be
useful to the general community.

\section{Failure Data Generation} \label{sec:generation}
This work extends the Adaptive Failure Prediction framework~\cite{irrera2015}
by conducting another case study with a Microsoft Windows Server acting as an
active directory service with a new implementation of the G-SWFIT technique for
the x86-64 architecture as well as a more representative fault load.  The case
study is then taken a step further by showing that the approach holds for other
predictors (maybe).  Specifically, findings are reported after implementing
this framework with the predictor used by Irrera, et al.~\cite{irrera2015} in
their case study and the predictor used by Watanabe, et
al.~\cite{watanabe2014}.

This section outlines the step-by-step procedure by which the extended AFP is
evaluated to show how effective it is when used on Windows Server deployments.
This is done by dividing the steps taken in an experiment into the three major
phases as defined in~\cite{irrera2015}: preparation phase, execution phase, and
training phase.

\subsection{Preparation Phase}
In this phase the AFP is prepared to run for the first time as described
in~\cite{irrera2015}.  The Cross Industry Standard Process for Data Mining
(CRISP-DM)~\cite{crispdm} should be applied to this situation when evaluating
how to best apply the AFP for a particular target.  For the purposes of this
research, our focus is on the Microsoft Windows Directory Services and
predicting failure in those services.  To demonstrate the efficacy of the AFP,
a predictor must be evaluated before and after a significant software update.
As a result, the most critical preparation made in evaluating this framework is
to hold back all software updates on the target system prior to the first run
of the execution phase.  

This phase is ultimately the implementation of the framework.  Each module of
the implementation for this work is detailed in
Section~\ref{sec:implementation} and is therefore not discussed further here.  

\subsection{Execution Phase}
A general outline of this process can be seen in
Figure~\ref{fig:ExecutionPhase}.  This phase is divided into three major
states: data collection and failure prediction, event checking, and
training/update as described in this section.

\figExecutionPhase

\subsubsection{Data Collection and Failure Prediction}
In this phase, the system has a working predictor providing input to some sort
of decision system.  It should be noted here that this decision system does not
have to be automated.  The system in this phase is making failure predictions
about the current state based on the last run of the training phase.  For the
Air Force application, an operator will be the decision maker and the output of
the AFP will be a message to that operator.  

\subsubsection{Event Checking}
Concurrent with the data collection and failure prediction sub-phase, the AFP
continuously monitors events that may alter the underlying system.  For this
experiment, these events are software updates.  The output of each episode of
this phase is a binary decision to either begin the training phase, or not.  In
this experiment, the training phase is manually triggered upon completion of a
major software update.

\subsubsection{Failure Predictor (Re-)Training and Update}
The purpose of this sub-phase is to initiate the training phase and compare its
results (a new predictor) with the currently employed predictor.  Should the
new predictor perform better, the old predictor is replaced by the new.

\subsection{Training Phase}
The training phase is broken down into five major steps:  target replication,
data generation \& collection, dataset building, predictor training, and
analysis.  The general flow can be seen in Figure~\ref{fig:TrainingPhase}.
Each phase is outlined in the following sub-sections.

\figTrainingPhase

\subsubsection{Target Replication}
During this phase a virtual clone of the target is made.  After the clone is
made, the fault injection and monitoring software must be installed.  In this
experiment, the monitoring tool is the same as the production system but care
must be taken to ensure the host-name is changed so the log messages generated
during this phase are not confused with messages from the production system.

\subsubsection{Data Generation \& Collection}
The purpose of this phase is to generate the data to train a new prediction
algorithm.  As a result, this sub-phase must be executed several times to
generate statistically meaningful datasets.  In this phase, the controller
triggers the cloned target startup.  Once startup is complete and the system
enters an idle state, the monitoring tool begins collecting data from the
target.  After monitoring has begun, the workload is started.  Once the
workload has entered a steady state, the fault load is started.  Finally, when
failure occurs, monitoring stops, the workload stops, and the system is
rebooted for the next run.  To generate golden data, the first run omits the
fault injection step.

The most critical part of this process is labelling the data when failure
occurs.  For the purposes of this experiment, D-PLG reports when authentication
fails and transmits a syslog message to the controller.  For this research,
failure has been defined by two criteria, the first is when authentication with
known good credentials fails.  The second is classified by when authentication
with known good credentials takes longer than thirty seconds (and it can then
be assumed that the service has crashed).

\subsubsection{Dataset Building}
In this phase, the raw syslog messages are formatted and encoded to train the
predictor.  The messages generated by the load generator are removed and in
each case the neighboring message with the closest timestamp is labelled as
when the failure occurred.

Irrera, et al.~\cite{irrera2015} loaded all event messages into a database for
processing.  In this work, the events are initially stored in a flat file on
the Ubuntu machine by the syslog daemon.  During this phase, a tool is run that
divides each of the $n$ event messages into $n$ observations with the following
three features: failure, timestamp, and message.  The failure feature is a
binary bit set if the event occurred within a configurable time window $\delta
t$ before the actual failure as defined by Salfner, et
al.~\cite{salfnerSurvey}.  The messages are processed by assigning a unique
integer for each unique word in the log messages.  The integers are then
concatenated resulting in a single feature (TODO: Better process?).  The
features for each observation are then combined in an $n \times 3$ matrix for
training.

\subsubsection{Predictor Training}
The purpose of this phase is to use the data generated by the forced failure of
the virtual clone to train a machine learning algorithm to classify a system as
failure prone or not.  

During this phase, each of the $k$ datasets produced by the $k$ runs of the
execution phase, each containing a single failure, are used to train a support
vector machine.  Each dataset is an $n \times 3$ matrix where $n$ is the
number of events recorded in the syslog for each run of the execution phase.
These $k$ datasets are used to conduct a $k$-fold cross validation training
and evaluation process where the first $k - 1$ datasets are used to train a
support vector machine.  The remaining set is used to validate the trained
model.  The data is then rotated and repeated $k$ times.

At each phase, the number of events that are correctly classified as failure
prone and occurring within the specified time window $\delta t$ are recorded as
well as the number of events incorrectly classified as failure prone occurring
within the time window.  These figures are then used to calculate the measures
of performance in the \emph{Analysis} phase.

\subsubsection{Analysis}
During this phase, the precision, recall, and area under the ROC curve are
computed using the figures measured in the previous phase so that the new
predictor can be compared against the old.  In this experiment, the focus is on
the precision and recall of the algorithm.  Since failure is such a rare event,
accuracy is not a very meaningful measure of performance and thus is not used.

\section{Implementation of the AFP} \label{sec:implementation}
\subsection{AFP Framework Implementation}
This experiment replicates the experiment in~\cite{irrera2015} except in place
of the web-server a Microsoft (MS) Windows Server running Active Directory (AD)
Domain Services.  In addition, several extensions to the original experiment
are made and presented here.  Multiple prediction techniques have been applied
using this framework to further generalize and validate the framework.  The
original AFP architecture is shown in Figure~\ref{fig:annotatedAFP} with the
parts that are modified in this work highlighted.  

\subsection{AFP Modules}
In~\cite{irrera2015}, the authors outline multiple modules into which they have
broken the AFP Framework for organizational purposes.  This research does not
modify these modules, instead, it takes a more granular approach and presents a
modified architecture and details each element of that architecture.

\figannotatedAFP  

The following sections detail the virtual environment in which this
architecture was constructed.  For reference, this virtual environment was
hosted on two VMWare ESXi 5.5 hypervisors each with two 2.6 GHz AMD Opteron
4180 (6 cores each) CPUs and 64 GB memory.  The individual virtual machines are
described in Tables~\ref{tab:hyp1}, and \ref{tab:hyp2}.

\tabHypervisorOne
\tabHypervisorTwo

\setcounter{secnumdepth}{5}

\subsection{Controller Hypervisor} \label{sec:controller} % 3.1.1
The controller functions in this experiment are split between two systems on a
single hypervisor seen on Table~\ref{tab:hyp2}.  One system is a Microsoft
Windows Server responsible for workload management and fault injection
management.  The additional Windows server also hosts remote desktop services
to allow the load generator to execute third party authentication with the
domain controller.  The other system is an Ubuntu 14.04 LTS server that
performs the failure prediction management and event management.  Each of
these functions is detailed in the following sections.

\subsubsection{Failure Prediction} \label{sec:failurePrediction} % 3.1.1.1
The failure prediction module predicts failure using machine learning
algorithms trained using the labelled training data generated by the rest of
this framework.  This module is constantly either training a new predictor
because a software update occurred, or predicting failure based on log messages
and other features produced by the production system.

THe AFP failure prediction function as outlined in~\cite{irrera2015}, is
performed by a support vector machine (SVM) predictor using \emph{libsvm}.
Additionally, the original experiment made use of a database that stored the
features and observations used for the failure prediction training algorithm.
This experiment does not modify the failure prediction module drastically as it
has already been shown in previous work that the online failure prediction area
of study is well explored~\cite{salfnerSurvey}.  This research makes use of a
different tool-set to execute the training and predicting phases.  Due to its
widespread use in the statistical community, the prediction and training
algorithms make use of the R programming language.

In this experiment, an SVM predictor is trained as done in~\cite{irrera2015},
as well as a... TODO maybe the method in~\cite{watanabe2014}?
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TO DO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%
% Need more details on actual prediction here.  Results of 623 paper will go  %
% here.                                                                       %
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%

\subsubsection{Fault Injection} \label{sec:faultInjectionMgr}
This module is responsible for managing the fault injector installed on the
clone of the production target system.  The purpose of the fault injector is to
force a loaded software application to fail in a realistic way so that the
indicators of that failure can be used to train the failure prediction
algorithm.

Irrera, et al.~\cite{irrera2015} use a tool implementing the G-SWFIT technique
for this module.  G-SWFIT was developed at the University of Coimbra in Coimbra
Portugal by Joao Duraes and Henrique Madeira~\cite{gswfit}.  The method is
widely implemented for use in software fault injection both commercially and
academically~\cite{natella2010,irrera2014,cotroneo2012,umadevi2015}.  Recently,
studies have questioned the representativeness of the failures generated by
G-SWFIT~\cite{kikuchi2014}.  In each case, the workload generated was critical
in creating representative faults.  This concern has been addressed in this
research and is discussed in Section~\ref{sec:workloadMgr}.

An additional concern has been that some faults that have been injected by use
of the G-SWFIT technique may not elude modern software testing and as a result
never actually occur in production software~\cite{natella2010}.  The
recommended remedy is to conduct source code analysis to determine which
pieces of code get executed most frequently and avoid fault injection in those
areas since they are most likely to be covered by unit tests.  Unfortunately,
the target is not an open source project and as a result, some of the faults
and resulting failures may never happen in a production environment.
Fortunately, the tool that has been developed for this research to do software
fault injection automatically scans each library loaded by the target
executable for fault injection points and then is capable of evenly
distributing the faults it does inject.

This work introduces an x86-64 implementation of the G-SWFIT technique called
W-SWFIT for Windows Software Fault Injection Tool.  The source code for W-SWFIT
has been published as open source on
Github\footnote{\url{https://github.com/paullj1/w-swfit/}} so that others may
use it for any of the reasons cited in the original G-SWFIT
paper~\cite{gswfit}.  For this research, the original plan was to use the same
tool used in~\cite{irrera2015} for fault injection.  Unfortunately, that tool
and all prior G-SWFIT implementations were incapable of injecting faults into
x86-64 binary executables.  Further, many of the commercial products that were
evaluated for this research were incapable of dealing with modern address space
layout randomization (ASLR).  As a result, W-SWFIT was developed for this
research and is capable of injecting faults into all user and kernel mode
applications on modern MS Windows operating systems.  

The key contributions of W-SWFIT are ASLR adaption and the x86-64 translation
we have performed.  G-SWFIT works by scanning binary libraries already in
memory for patterns (or operators) that match compiled errors in software
development.  The faults were based on the Orthogonal Defect
Classification~\cite{bridge1998} and can be seen in~\ref{tab:faults}.  As
pointed out in~\cite{salfnerSurvey} and~\cite{gswfit}, failures are ultimately
the result of software developer errors.  Unfortunately, much of the work done
in~\cite{gswfit} was on encoding common development errors as IA32 assembly
instructions so that working binary executable code could be mutated in memory
to introduce these errors in running applications.  The target application in
this research is strictly an x86-64 (also known as x64 or amd64) application
and the patterns identified in~\cite{gswfit} are incompatible.  Consequently, a
fault injection tool capable of mutating x86-64 instructions in the same way
was required.  W-SWFIT implements many of the operators in~\cite{gswfit} in
the x86-64 language by translating the operators seen in Table~\ref{tab:faults}
from IA32 to x86-64.  A simple example of this translation is shown on the
entry/exit points of a function in Tables~\ref{tab:translationThirtyTwo},
and~\ref{tab:translationSixtyFour}.  The rest of the translations can be seen
in source code for W-SWFIT.  In many cases, the translation was very simple,
but in others, the IA32 patterns did not cleanly map to x86-64 byte code.  When
this happened, great care was taken to ensure the pattern was correctly mapped
to x86-64.

\tabFaults
\tabTranslationThirtyTwo
\tabTranslationSixtyFour

\subsubsection{Workload Managmement} \label{sec:workloadMgr} 
The Workload module creates realistic work for the target system in the sandbox
hypervisor to do as a way of generating computational load.  Without this
module, it could take a very long time for an injected fault to manifest itself
as a failure.  Consider a missing \emph{free} statement and the consequent
memory leak.  A production target server may have a considerable amount of
memory and the leak could be very small.  To accelerate the possibility of
failure occurring, realistic load must be generated against the sandbox clone
of the production target.

In the original AFP case study, a Windows XP based web-server was used for a
target and therefore the load generation was done by a simple web request
generator.  As previously mentioned, realistic workload is critical in
generating realistic failure and consequently training a useful predictor.  As
a result, much emphasis is been placed on this module.  Since the target is
not a web server, it was not possible to use the same load generator as was
used in~\cite{irrera2015}.  Initial searches for a load generator suitable for
this research yeilded a tool developed by Microsoft that initiated remote
desktop connections to aid in sizing a terminal services
server\footnote{\url{http://www.microsoft.com/en-us/download/details.aspx?id=2218}}.
By executing a remote desktop session, the authentication and DNS functions of
the domain controller would also be loaded.  Unfortunately, this tool is no
longer maintained and would not execute on the target
machine\footnote{\url{https://social.technet.microsoft.com/Forums/windowsserver/en-US/2f8fa5cf-3714-4eb3-a895-c30e2b26862d/debug-assertion-failed-sockcorecpp-line-623}}.
Further searches for tools that would sufficiently load the domain controller
did not produce any results.  Consequently, a tool to produce realistic load
for a domain controller was developed for this research and is introduced here.

%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TO DO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%
% If D-PLG paper gets published, this should change                           %
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%

The Distributed PowerShell Load Generator (D-PLG) is a collection of Microsoft
PowerShell scripts designed to generate realistic traffic that will
sufficiently load a Microsoft domain controller.  Other network traffic
generators typically work by replaying traffic captured on a live network.
Unfortunately, due to the cryptographic nature of authentication, simply
replaying traffic will not load a service since the timestamps and challenge
responses will no longer be valid.  As a result, any replayed traffic will be
dropped and ignored by a live domain controller.  D-PLG solves this problem by
making native authentication requests by use of built-in PowerShell cmdlets
(command-lets).  By doing this, realistic authentication requests are sent to
the domain controller and are actually processed.  The functions performed by
the domain controller have been evaluated and D-PLG is designed to sufficiently
load each of the services responsible for performing those functions.
functions.

In this experiment, the DC is configured as it is in the Air Force Network
architecture.  After careful analysis, it has been determined that the major
roles in the Air Force architecture being performed are authentication and
domain name service (DNS).  Additionally, by use of native cmdlets, D-PLG is
capable of generating four kinds of traffic: web, mail, file sharing, and MS
remote desktop protocol (RDP).  D-PLG uses the MS Powershell environment to
generate the traffic in an effort to make the traffic as real as possible.
After building the tool, an experiment was constructed and executed on a scale
model of a production environment.  The scaled simulation network was built
using the recommendations of the Microsoft community for sizing a domain
controller~\cite{mak12} and tested by running the tool on five client machines
against the domain controller for five rounds of five minutes.  The results of
this test can be seen in Figures~\ref{fig:authDCPPS}, \ref{fig:authClientPPS},
\ref{fig:authDCMetrics}.

\figAuthDCPPS
\figAuthClientPPS
\figAuthDCMetrics

D-PLG makes use of client machines running a Windows operating system with
PowerShell version 4.0 or newer.  The controller asks each machine to generate
a configurable list of requests at evenly spaced intervals for a configurable
duration of time.  While this may not be realistic network traffic, it will
produce realistic load against a domain controller.  Since D-PLG depends on the
use of client machines, it is recommended that any load generation be conducted
during off-peak hours if spare client sized machines are not available.  It
should be noted however, that even with poorly resourced client machines (seen
in~\ref{tab:hyp1}), D-PLG was able to generate fifteen thousand authentication
sessions over a five minute period; approximately 10 authentication sessions
per machine, per second.  With modern workstations, the impact on these client
machines will likely be negligible and could likely be in use during load
generation.

Based on these results, and that a production domain controller should be at
approximately 40\% CPU utilization during peak utilization~\cite{mak12}, we
have concluded that D-PLG is capable of sufficiently loading the domain
controller over a sustained period of time for the purposes of implementing the
AFP framework and is used in this research.  Further, we have concluded that
D-PLG is capable of scaling to provide load against higher capacity domain
controllers by using only a few client machines.  There are many more uses for
a load generator of this type and we were not able to find a tool that is
capable of creating the same type of load so we have published these scripts on
Github\footnote{https://github.com/paullj1/AFP-DC/tree/master/D-PLG} for others
to use.

\subsubsection{Events Manager} \label{sec:eventsManagerMgr}
This module is responsible for receiving and managing log messages and other
events that may be used to train the failure prediction algorithm.  Irrera, et
al.~\cite{irrera2015} use the \emph{Logman} tool for event management in their
original case study.  Since the experimental environment was modelled after the
Air Force enterprise environment, the \emph{Solar Winds} log forwarding tool is
used to perform the functions in this module as it is already present on many
of the Air Force domain controllers.  The domain controllers on the Sandbox and
Target hypervisors forward all events to the Ubuntu virtual machine with the
\emph{rsyslog} server daemon configured to receive all messages.  These
messages are then processed and added to a SQL database for training and
prediction.  

\subsubsection{Sandbox Management} \label{sec:sandboxMgr} 
The purpose of the sandbox management module is to supervise the virtual
cloning of the production system that is made when a new predictor is to be
trained.  As Irrera, et al.~\cite{irrera2015,irrera2013} point out, it is
typically inappropriate to inject faults and cause failures in production
systems, so a virtual clone must be created for that purpose.

The sandbox is managed manually using Virtual Machine (VM) snapshots.  After an
initial stable state was configured, snapshots of every component of the
architecture were taken so that they could be reset after iterations of the
experiment.  It is important to note here that because VMWare has documented
APIs, in future work, this function could be automated.

\subsection{Sandbox Hypervisor} \label{sec:sandbox}
The sandbox is constructed on a single hypervisor implemented as seen on
Table~\ref{tab:hyp1}.  The following sections outline each module within this
module.

\subsubsection{Fault Injection} \label{sec:faultInjectionTool} 
This module is responsible for causing the target application to fail so that
labelled failure data can be generated in a short period of time.  As described
in Section~\ref{sec:faultInjectionMgr}, W-SWFIT has been developed to serve
this purpose and implements the G-SWFIT technique developed by Duraes, et
al.~\cite{gswfit} for fault injection.  The execution is controlled by the
Windows Server virtual machine on the Controller hypervisor through PowerShell
remote execution to reduce the interaction and potential to introduce bias into
the training data.  The tool allows us to inject a comprehensive list of faults
into the AD Services processes and binary libraries which are mostly contained
within the `lsass.exe' process.  Since many of the critical functions performed
by the AD Services processes are performed in one library called `ntdsa.dll',
it is the focus of fault injection.

This function is extended by this research to include failure as a result of
excessive load and failure as a result of a corrupt database.
Section~\ref{sec:extensions} covers these extensions in more depth.

\subsubsection{Monitoring} \label{sec:sandboxMonitoringTool} 
The purpose of this module is to capture some evidence or indication of pending
failure so that it may be used to train a prediction algorithm.  Irrera, et
al.~\cite{irrera2015} use the \emph{Logman} tool in their original study but
because the experimental infrastructure used in this research is modelled after
production Air Force networks, the \emph{Solar Winds} log forwarding tool is
used because it is already present in the Air Force architecture.  The tool is
a lightweight application that simply forwards windows events to a syslog
server.

\subsubsection{Sandbox Workload}  \label{sec:sandboxWorkload} 
The sandbox workload module is likely the most critical module in the entire
framework.  Its purpose is to create realistic work for the target application
to do before faults are injected.  If the workload is not realistic, then the
failures that occur after fault injection will not be representative of real
failures and any data or indicators collected cannot be used to train an
effective prediction algorithm.

Irrera, et al.~\cite{irrera2015} used a web traffic generator called TPC-W
installed on a single machine in their original study because their target was
a web server.  Because the domain controller does not respond to web-requests
and a tool had not previously been written for this application, a tool was
developed for this research called D-PLG. D-PLG is a tool that generates
approximately ten full-stack authentication sessions requests per second in
order to sufficiently load the domain controller.  D-PLG is a distributed tool
and requires the use of client machines as a result.  This module is
represented by those client machines.  In this experiment, the client portion
of D-PLG is installed on five client machines managed by the central workload
manager as discussed in Section~\ref{sec:workloadMgr}.

\subsection{Target Hypervisor} \label{sec:target}
The target hypervisor was constructed as a clone of the sandbox hypervisor seen
on Table~\ref{tab:hyp1}.  The following section outlines the monitoring tool
installed on the domain controller on this hypervisor.  It should be noted here
that while the client machines were cloned as well for convenience, they were
not used in this experiment.

\subsubsection{Monitoring} \label{sec:targetMonitoringTool}
The monitoring module is exactly the same as the sandbox monitoring module and
for this experiment, the \emph{Solar Winds} syslog forwarding tool is used.  To
ensure that the messages that are sent are uniquely identified by the
controller, the hostname of the target machine must be different from the
hostname of the sandbox target machine.

\setcounter{secnumdepth}{3}

\section{Extenstions to the AFP} \label{sec:extensions}
This section outlines a few additional extensions to the Adaptive Failure
Prediction Framework specifically with respect to the fault load.  One
additional type of failure that may be used to train a predictor can be
generated by under-allocating resources for the domain controller in the
sandbox hypervisor.  Under some circumstances, this may not be considered a
realistic form of failure.  However, one reason an organization may want to
implement the AFP may be that monetary resources are not available to implement
an adequately redundant domain controller and as a result, it may be possible
that an adequately sized domain controller is not an option.  Consequently,
load based failure may be a realistic challenge faced by some organizations and
knowing that such a failure may occur might be valuable.

Another type of failure that the extended architecture evaluates goes one layer
deeper in analyzing the function performed by the target application.  In
addition to targeting the main library that performs authentication on a domain
controller with fault injection, the extended framework will also target the
library responsible for interacting with the user database.  In this way, the
extended AFP framework is capable of simulating a corrupted database that may
occur as a result of a corrupted sector on the disk where the database is
stored.

By adding these two additional types of failures to the data used to train a
prediction algorithm, the resulting algorithm will be able to predict a wider
range of realistic failures.

