\section{Experimental Results and Analysis} \label{chapter4}
This section reports results after conducting the experiments laid out in
Section~\ref{chapter3}.  First, common reporting techniques and measures of
performance are reviewed.  These measures and reporting techniques are then
used to report the results of the experiments conducted.  The section concludes
with a short summary.

\subsection{Performance Measures} \label{metrics}
This section reviews the performance measures used in this section to
demonstrate the efficacy and quality of the statistical models trained in this
research.  These measures are commonly used in the field of machine learning to
compare and assess predictors and are taken from a survey of \ac{OFP} methods
written by \citet{salfnerSurvey}.

This research utilizes a technique called cross-validation in which a set of
labelled training data are broken into three parts as follows:
\begin{enumerate}
\item{Training Set:  A data set that allows a prediction model to establish and
optimize its parameters}
\item{Validation Set:  The parameters selected in the training phase are then
validated against a separate data set}
\item{Test Set:  The predictor is finally run against a final previously
unevaluated data set to assess generalizability}
\end{enumerate}
During the test phase, true positives (negatives) versus false positives
(negatives) are determined in order to compute the performance measures in this
section.  The following terms and associated abbreviations are used: \ac{TP} is
when failure has been predicted and then actually occurs; \ac{FP} is when
failure has been predicted and then does not occur; \ac{TN} is when a state has
been accurately classified as non-failure prone; \ac{FN} is when a state has
been classified as non-failure prone and a failure occurs.

\subsubsection{Precision and Recall}
Precision and recall are the most popular performance measures used when
for comparing \ac{OFP} approaches.  The two are related and often times
improving precision results in reduced recall.  Precision is the number of
correctly identified failures over the number of all predicted failures.  In
other words, it reports, out of the predictions of a failure-prone state that
were made, how many were correct.  In general, the higher the precision the
better the predictor.  Precision is expressed as:

\[ Precision 
	= \dfrac{TP}{TP + FP} \in [0,1]
\]

Recall is the ratio of correctly predicted failures to the number of true
failures.  In other words, it reports, out of the actual failures that
occurred, how many the predictor classified as failure-prone.  In conjunction
with a higher precision, higher recall is indicative of a better predictor.
Recall is expressed as:

\[ Recall 
	= \dfrac{TP}{TP + FN} \in [0,1]
\]

F-Measure is the harmonic mean of precision and recall and represents a
trade-off between the two \citep{rijsbergen1979v}.  A higher F-Measure reflects
a higher quality predictor.  F-Measure is expressed as:

\[ F\mhyphen Measure 
	= \dfrac{2 \cdot Precision \cdot Recall}{Precision + Recall} \in [0,1]
\]

\subsubsection{Precision/Recall \& \ac{ROC} Curves}
Much like with other predictors, many \ac{OFP} approaches implement variable
thresholds to sacrifice precision for recall or vice versa.  That trade-off is
typically visualized using a precision/recall curve.  Another popular
visualization is to plot \ac{TPR} over \ac{FPR}.  This produces the \ac{ROC}
curve.  The \ac{ROC} curve relationship can be further illustrated by
calculating the \ac{AUC}.  A purely random predictor will result in an \ac{AUC}
of $0.5$ and a perfect predictor a value of $1$.  The \ac{AUC} can be thought
of as the probability that a predictor will be able to accurately distinguish
between a failure-prone state and a non-failure-prone state, over the entire
operating range of the predictor.

The results of the experiments conducted in this research report all of the
above described measures of performance in the next section.

\subsection{Results} \label{results}
The experiments designed in Section~\ref{chapter3} were executed in a virtual
environment to produce failure data.  The failure data generated was used to
train statistical learning models using the open source statistical learning
software suite: \emph{R}.  The parameters used to train each model were
selected using cross-validation on a subset of the failures generated.
Finally, each model was evaluated using a held-out test set.  The results of
this evaluation for each fault load are reported here.

The rest of this section is organized first by the target system, then by the
different fault loads that were used to generate failure data on the
corresponding target.  In each sub-section, the results after training a
machine learning model on failure data generated using that type of fault are
detailed.  Finally, this section is concluded with a summary of these results.

\subsubsection{\ac{MS} \ac{DC}}
\paragraph{Fault Injection}
This fault load was effective at creating a failure, but unfortunately, each
failure observed occurred immediately after introducing the fault.  Because
there was no delay between injection and failure, there did not exist any
indicators of failure.  Consequently, machine learning cannot help in this
situation.  According to \citet{russinovich2009} the
\emph{lsass.exe} process, as well as other critical system processes, are at
the top of the structured exception handling stack and do not handle
exceptions.  When faced with exceptions, the processes exit and the system
reboots.

\paragraph{Under-Resourced \ac{CPU}}
This fault load never resulted in failure.  To test this fault load, the
virtual domain controllers resources were reduced.  The \ac{CPU} went from a
dual-core to a single virtual CPU, and the memory was reduced from $2$ Gb to
$512$ Mb.  This reduction was well beneath the recommended capacity for a
domain controller \citep{mak12}.  The workload generator was then allowed to
run against this configuration for seven days.  For the duration of the test,
the \ac{CPU} load was $100\%$, and physical memory was $90\%$ utilized on
average.  While the service did experience reduced response time, failure did
not occur.

Another test was conducted to test this fault load by allowing a third-party
application to slowly consume all \ac{CPU} time.  Much like the previous test,
this test never resulted in failure.  Consequently, the learning was not
attempted for fault load.

\paragraph{Under-Resourced Memory}
The under-resourced memory fault load was the first that created observable
indicators of failure with any lead time.  This fault load produced the best
performing predictors and the largest sliding time window for prediction of
sixty seconds.  According to \citet{islr}, there can be advantages and
trade-offs between parametric and non-parametric models.  For this reason, this
experiment explores the use of two machine learning models: the weighted
\ac{SVM}, and boosted decision trees using the multinomial distribution.  

\paragraph{Weighted \ac{SVM}}
For this prediction method, the \emph{e1071} package was used to train an
\ac{SVM}.  The \emph{tune} function was used to run a $5$-fold cross-validation
a total of $48$ times to select the optimal parameters (gamma, cost, and degree
polynomial) using: four kernels, four sliding data/prediction windows, and
three training/test data splits.  The classification weights were set to
roughly equal the proportion of failure prone to non-failure prone data windows
$0.8$ for failure, and $0.2$ for non-failure.

The optimal model was selected with the Radial kernel with $\gamma = 0.1$,
$cost = 1$, time window $= 60$ seconds, and the split of data $= 4$ of the
observed failures used for training.

Initial test performance was poor so the test data was then evaluated in
sequential order using a threshold.  After two sequential windows were
predicted as failure-prone, the next $w$ windows were also predicted as
failure-prone, where $w = $ \emph{window size} $-$ \emph{threshold size}.  For
threshold $= 2$, the resulting confusion matrix for the optimal F-Measure, the
\ac{ROC} curve, and the precision/recall curve are shown in
Table~\ref{tab:memLeakPreUpdateSVMConfusionMatrix}, and
Figure~\ref{fig:memLeakPreUpdateSVMPerf} respectively.

\figMemLeakPreUpdateSVMPerf
\tabMemLeakPreUpdateSVMConfusionMatrix

After the software update, the same model was used on a new set of generated
failures.  The old model did not accurately classify a single failure prone
time window.  A new model was then trained with the newly generated failure
data.  Unfortunately, after this software update, with all other things held
constant, the weighted SVM model was unable to achieve the same level of
performance as before.

\paragraph{Boosted Decision Trees}
For this prediction model, the \emph{gbm} package was used to train a boosted
decision tree.  Cross-validation was used to select $\lambda = 0.03$, the
interaction depth of $= 4$, and the number of trees $= 1000$.  The multinomial
distribution was used to perform classification.  This was chosen instead of
Bernoulli given that the two distributions are the same except multinomial is
capable of classification with more than two classes.  While this flexibility
is not required for this experiment, it may be useful in the future to predict
additional system states like `degraded', or `idle'.

The precision/recall, and \ac{ROC} curves on a sixty second data/prediction
window are shown in Figure~\ref{fig:memLeakPreUpdateBoostingPerf}.  The
confusion matrix at the optimal threshold for F-measure is shown in
Table~\ref{tab:memLeakPreUpdateBoostingConfusionMatrix}.

\figMemLeakPreUpdateBoostingPerf
\tabMemLeakPreUpdateBoostingConfusionMatrix

After the software update, the same prediction model was used new set of
generated failures.  The precision/recall and \ac{ROC} curves on data generated
after the software update using the old model are shown in
Figure~\ref{fig:memLeakPostUpdateSameBoostedModel}.  The confusion matrix at
the optimal threshold for F-measure is shown in
Table~\ref{tab:memLeakPreUpdateBoostingConfusionMatrix}

\figMemLeakPostUpdateSameBoostedModel
\tabMemLeakPostUpdateBoostingSameModelConfusionMatrix

Finally, a new predictor was trained using more generated failures as was done
before the update.  The precision/recall, and \ac{ROC} curves on the held-out
test data are shown in Figure~\ref{fig:memLeakPostUpdateBoostingPerf} and the
confusion matrix at the optimal threshold for F-measure is shown in
Table~\ref{tab:memLeakPostUpdateBoostingConfusionMatrix}.

\figMemLeakPostUpdateBoostingPerf
\tabMemLeakPostUpdateBoostingConfusionMatrix

\paragraph{Heap Space Corruption}
Just as with fault injection, this fault load was able to produce failures, but
these failures were not preceded by any indicators.  To increase realism in
this fault load, the focus of the corruption was on the user database.  The
user database is incrementally cached as authentication requests are
received \citep{russinovich2009}.  To test this fault load, the \ac{AFP}
execution phase was run as normal.  After the workload generator reached a
steady state, a single user in the database on disk was corrupted followed
immediately by the same user being corrupted in process memory.  If the disk
was not corrupted along with memory, the process would treated the corruption
as a cache miss, and re-cached the user from disk.  When both were corrupted
simultaneously, the process crashed and forced system reboot the very next time
that user requested authentication.  Unfortunately, exactly as with fault
injection, there were no preceding indicators of failure and thus training a
prediction model was unsuccessful.

\subsubsection{Web Server}
To validate the approach and implementation of the \ac{AFP} framework in this
experiment, it was also tested against an Apache web server.  The underlying
system change in this experiment was simulated by upgrading Apache from version
\emph{2.2.31 x64} to version \emph{2.4.20 x64}.  Results for the web server
were almost identical to those for the \ac{DC} for each fault load.  The only
predictable failure was in the case of the memory leak.  The following
sub-sections outline specific results after testing each fault load.

\paragraph{Fault Injection}
In the case of the web server, each library loaded by the Apache server process
\emph{httpd.exe} was targeted for fault injection.  In every case, faults were
injected until failure occurred.  Much like the \ac{DC}, for each failure
observed, no preceding indications of failure were visible in the log messages.

\paragraph{Under-Resourced CPU}
Much like with the \ac{DC}, both methods of creating this situation resulted in
no failure.  The client machines did experience delayed responses, but the
server continued to run.

\paragraph{Under-Resourced Memory}
As with the \ac{DC}, this was the only fault load that could be used to predict
failure given only reported errors.  However, machine learning was not
necessary given the low number of log messages produced.  Since Apache stores
access requests in a separate file, they were essentially pre-filtered.  Apache
also by default, stores error messages in an external log.  There were no
messages reported in this file in any of the failure runs conducted.  The only
indicators produced, were reported by Windows and recorded by the rsyslog
server.  An average number of $15$ messages were reported during each round of
the execution phase and the indicators of failure were easy to see.  In this
case, simple rules could be used to predict failure in this process so a
learning algorithm was not trained.  

After the Apache software update was applied, the indicators of failure did not
change and there were no additional messages reported in the separate error
log.  For this reason, the same updates were applied to the operating system as
was done for the \ac{DC} target.  After these updates, the indicators changed
slightly but were still very few and could be used to write a few simple rules.

These results do not diminish the utility of the \ac{AFP} framework.  Without
the framework, the indicators would still be unknown until after a failure.
Moreover, there would be no way to tell how long a set of rules would be
effective after being written.

\paragraph{Heap Space Corruption}
This fault load was tested against the Apache server by targeting the actual
web page stored in memory.  Much like was done by the \ac{DC} with users, this
was treated as a cache miss and the content was retrieved from disk.  Again, to
simulate a disk failure, this file was made inaccessible.  The result was an
immediate failure to serve the content.  As with the \ac{DC}, there were no
preceding indications of failure.

\subsubsection{Summary}
In summary, the only fault load usable for training a statistical model to
predict failure based only on reported errors was the memory leak.  As
expected, the software update did drastically reduce the effectiveness of a
model trained with failure data before the software update.  The boosted
decision tree was able to be re-trained after the software update, but the
\ac{SVM} was not.  This suggests that both models should be used to ensure the
\ac{AFP} framework is able to adapt to the underlying system changes and
maintain at least one useful predictor.
