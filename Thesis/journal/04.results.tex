\section{Experimental Results and Analysis} \label{chapter4}
To test the extended \ac{AFP} framework, failure data was generated before a
series of major software updates using software fault injection,
under-resourced \ac{CPU}, under-resourced memory, and heap space corruption, on
two Windows Server 2008 machines: the \ac{DC}, and the Apache web server.  The
failure data was used to train two statistical prediction models: an \ac{SVM}
classifier, and a boosted decision tree.  Following the software updates, more
failure data was generated and the old statistical models were used to predict
failure in the new data.  Finally, new statistical models were trained using
the new data.  To compare each fault load both before and after the software
updates, performance was measured using the \ac{AUC} and F-Measure.

In general, the \ac{AFP} framework works by virtually cloning the target
production system after it has determined that system has changed.  In this
case, this determination is made after several important software updates.  The
framework then generates realistic work for the cloned service to perform,
which accelerates the activation of an injected fault.  When the cloned system
is sufficiently loaded, faults are injected until failure occurs.  Once failure
has occurred, the recorded data is used to train a statistical learning model.
The new model then replaces the existing model if it performs better.

Since log messages were used to train the statistical model, they needed to be
transformed to numerical data.  During execution, event messages were stored in
a flat file on the Ubuntu machine by the syslog server daemon in the
\emph{Snare}\footnote{\url{http://wiki.rsyslog.com/index.php/Snare\_and\_rsyslog}}
MSWinEventLog format.  The first element in each message is the time-stamp and
host name of the sender prepended by the syslog server daemon: \emph{May 8
14:31:52 dc.afnet.com}.  The remainder of the message contains tab delimited
values where the keys (and consequent features) are shown in
Table~\ref{tab:message}.  Of these features, Criticality, EventLogSource,
EventID, SourceName, and CategoryString were selected for further encoding.

\tabMessage

Events were filtered by EventID as is done by \citet{fulp2008} to reduce the
noise generated by successful login attempts.  Log messages with IDs shown in
Table~\ref{tab:messageIDs} were filtered from the input.  

Next, to encode the time dimension and reduce the sequential message ordering
dependency, a sliding time window was created by counting each unique entry for
each feature within the data window ($\mathrm{\Delta t_d}$)
\citep{vaarandi2002}.  During this stage, the number of messages that were
reported in the data window were also recorded and used as a feature.

Finally, each time window preceding the failure within $\mathrm{\Delta t_l}$
was labelled as failure prone \citep{irrera2015}.  This encoding enables the
use of classification algorithms in the training phase.  An example of the
final encoding is shown in Table~\ref{tab:window}.

\tabMessageIDs % Keep these together (footnote)
\footnotetext{\url{https://support.microsoft.com/en-us/kb/977519}}
\tabSlidingWindow

Feature reduction was performed for both learning algorithms on a sliding time
window \citep{fulp2008,irrera2013a,vaarandi2002}.  This transformed data was
then used to train \hl{weighted} \ac{SVM} and boosted decision tree models
using cross validation on $5$ recorded failure runs for each fault load for
both systems before and after the software updates.  \hl{These models were used
because of how well they tend to perform on unbalanced data.} Upon completion
of the data generation and model training, several performance measures were
calculated on held out test data.

\hl{It is important to note that the above described method of prediction only
provides a binary classification at a given time and only indicates that
failure will occur within the fixed time window.  Other methods of prediction
may offer more insight into when the failure may occur but they were not
explored in this work.  Furthermore, given the distinct differences in the
training and execution phases, careful consideration was made for real time
prediction.  During the Execution phase, this same process occurs by
maintaining a sliding time window by checking for new log messages every
second.}

\subsection{\acrfull{MS} \acrfull{DC} Results}
The \ac{MS} \ac{DC} was configured in the virtual environment to host a
30,000 user database and perform \ac{DNS} and authentication for all
workstations.  The target of the fault injection was the \emph{lsass.exe}
process, and specifically the \emph{ntdsa.dll} library.  This library is
responsible for processing authentication requests and handles interaction with
the user database.

\paragraph{Fault Injection}
Fault injection was effective at creating a failure, but unfortunately, each
failure observed occurred immediately after introducing the fault.  Because
there was no delay between injection and failure ($\mathrm{\Delta t_l \approx
0}$), there did not exist any indicators of failure.  Consequently, machine
learning cannot help in this situation.  According to \citet{russinovich2009}
the \emph{lsass.exe} process, as well as other critical system processes, are
at the top of the structured exception handling stack and do not handle
exceptions.  When faced with exceptions, the processes exit and the system
reboots.

\paragraph{Under-Resourced \ac{CPU}}
While this load resulted in authentication requests that took longer, this load
never led to failure.  To test this load, the virtual domain controllers
resources were reduced.  The \ac{CPU} went from a dual-core to a single virtual
CPU, and the memory was reduced from $2$ Gb to $512$ Mb.  This reduction was
well beneath the recommended capacity for a domain controller \citep{mak12}.
The workload generator was then allowed to run against this configuration for
seven days.  For the duration of the test, the \ac{CPU} load was $100\%$, and
physical memory was $90\%$ utilized on average.  While the service did
experience reduced response time, failure did not occur.

Another test was conducted to test this load by allowing a third-party
application to slowly consume all \ac{CPU} time.  Much like the previous test,
this test never resulted in failure.  Consequently, the learning was not
attempted for this load.

\paragraph{Under-Resourced Memory}
The under-resourced memory load was the first that created observable
indicators of failure with any lead time.  This load produced the best
performing predictors and the largest sliding time window for prediction of
sixty seconds.  For this reason, this experiment explores the use of two
machine learning models: the weighted \ac{SVM}, and boosted decision trees
using the multinomial distribution.  

\paragraph{Weighted \ac{SVM}}
For this prediction method, the \emph{e1071} package in R was used to train an
\ac{SVM}.  The \emph{tune} function was used to run a $5$-fold cross-validation
a total of $48$ times to select the best performing parameters (gamma, cost,
and degree polynomial) using: four kernels, four sliding data/prediction
windows, and three training/test data splits.  The classification weights were
set to roughly equal the proportion of failure prone to non-failure prone data
windows $0.8$ for failure, and $0.2$ for non-failure.

The best performing parameters were the Radial kernel with $\gamma = 0.1$, $c =
1$, time window $= 60$ seconds, and the split of data $= 4$ of the observed
failures used for training, with the remaining used for test.

Test data is evaluated in temporal order over two data windows.  The resulting
precision/recall and \ac{ROC} curves are shown in
Figure~\ref{fig:memLeakPreUpdateSVMPerf}.
Table~\ref{tab:memLeakPreUpdateSVMConfusionMatrix} shows the confusion matrix
on test data created before software updates on threshold with highest
F-Measure $= 0.8739$.

\figMemLeakPreUpdateSVMPerf
\tabMemLeakPreUpdateSVMConfusionMatrix

After the software update, the same model was used on a new set of generated
failures.  The old model did not accurately classify a single failure prone
time window.  A new model was then trained with the newly generated failure
data.  Unfortunately, after this software update, with all other things held
constant, the weighted SVM model was unable to achieve the same level of
performance as before resulting in a maximum F-Measure of $0.4380$.

\paragraph{Boosted Decision Trees}
For this prediction model, the \emph{gbm} package in R was used to train a
boosted decision tree.  Cross-validation was used to select $\lambda = 0.03$,
the interaction depth of $= 4$, and the number of trees $= 1000$.  The
multinomial distribution was used to perform classification.

The precision/recall, and \ac{ROC} curves on a sixty second data/prediction
window are shown in Figure~\ref{fig:memLeakCombinedBoostPerf}.  The
confusion matrix at the optimal threshold for F-measure is shown in
Table~\ref{tab:memLeakPreUpdateBoostingConfusionMatrix}.

\figMemLeakCombinedBoostPerf
\tabMemLeakPreUpdateBoostingConfusionMatrix
% Confusion matrix on test data created before software updates on
% threshold with highest F-Measure (0.9917) using boosting.

After the software update, the same prediction model was used new set of
generated failures.  The precision/recall and \ac{ROC} curves on data generated
after the software update using the old model are shown in
Figure~\ref{fig:memLeakCombinedBoostPerf}.  The confusion matrix at
the optimal threshold for F-measure is shown in
Table~\ref{tab:memLeakPreUpdateBoostingConfusionMatrix}

\tabMemLeakPostUpdateBoostingSameModelConfusionMatrix
% Post-update failure data confusion matrix on threshold with highest F-Measure
% (0.4691) using model trained on failure data generated before software
% update.

Finally, a new predictor was trained using more generated failures as was done
before the update.  The precision/recall, and \ac{ROC} curves on the held-out
test data are shown in Figure~\ref{fig:memLeakCombinedBoostPerf} and the
confusion matrix at the optimal threshold for F-measure is shown in
Table~\ref{tab:memLeakPostUpdateBoostingConfusionMatrix}.

\tabMemLeakPostUpdateBoostingConfusionMatrix
% Post-update failure data confusion matrix on threshold with highest F-Measure
% (0.9355) using model trained on failure data generated after software update.

In summary, before the software update, the boosted decision tree performed
using test data with an \ac{AUC} of $0.9984$.  After the software update, the
test \ac{AUC} dropped to $0.4854$ but was retrained to achieve an \ac{AUC} of
$0.9801$.

\paragraph{Heap Space Corruption}
Just as with fault injection, heap space corruption was able to produce
failures, but these failures were not preceded by any indicators.  To increase
realism, the focus of the corruption was on the user database.  The user
database is incrementally cached as authentication requests are received
\citep{russinovich2009}.  To test this load, the \ac{AFP} execution phase was
run as normal.  After the workload generator reached a steady state, a single
user in the database on disk was corrupted followed immediately by the same
user being corrupted in process memory.  If the disk was not corrupted along
with memory, the process would treated the corruption as a cache miss, and
re-cached the user from disk.  When both were corrupted simultaneously, the
process crashed and forced system reboot the very next time that user requested
authentication.  Unfortunately, exactly as with fault injection, there were no
preceding indicators of failure and thus training a prediction model was
unsuccessful.

\subsubsection{Web Server}
To validate the approach and implementation of the \ac{AFP} framework in this
experiment, it was also tested against an Apache web server.  The underlying
system change in this experiment was simulated by upgrading Apache from version
\emph{2.2.31 x64} to version \emph{2.4.20 x64}.  Results for the web server
were almost identical to those for the \ac{DC} for each load.  The only
predictable failure was in the case of the memory leak.  The following
sub-sections outline specific results after testing each load.

\paragraph{Fault Injection}
In the case of the web server, each library loaded by the Apache server process
\emph{httpd.exe} was targeted for fault injection.  In every case, faults were
injected until failure occurred.  Much like the \ac{DC}, for each failure
observed, no preceding indications of failure were visible in the log messages.

\paragraph{Under-Resourced CPU}
Much like with the \ac{DC}, both methods of creating this situation resulted in
no failure.  The client machines did experience delayed responses, but the
server continued to run.

\paragraph{Under-Resourced Memory}
As with the \ac{DC}, this was the only load that could be used to predict
failure given only reported errors.  However, machine learning was not
necessary given the low number of log messages produced.  Since Apache stores
access requests in a separate file, they were essentially pre-filtered.  Apache
also by default, stores error messages in an external log.  There were no
messages reported in this file in any of the failure runs conducted.  The only
indicators produced, were reported by Windows and recorded by the rsyslog
server.  An average number of $15$ messages were reported during each round of
the execution phase and the indicators of failure were easy to see.  In this
case, simple rules could be used to predict failure in this process so a
learning algorithm was not trained.  

After the Apache software update was applied, the indicators of failure did not
change and there were no additional messages reported in the separate error
log.  For this reason, the same updates were applied to the operating system as
was done for the \ac{DC} target.  After these updates, the indicators changed
slightly but were still very few and could be used to write a few simple rules.

These results do not diminish the utility of the \ac{AFP} framework.  Without
the framework, the indicators would still be unknown until after a failure.
Moreover, there would be no way to tell how long a set of rules would be
effective after being written.

\paragraph{Heap Space Corruption}
Heap space corruption was tested against the Apache server by targeting the
actual web page stored in memory.  Much like was done by the \ac{DC} with
users, this was treated as a cache miss and the content was retrieved from
disk.  Again, to simulate a disk failure, this file was made inaccessible.  The
result was an immediate failure to serve the content.  As with the \ac{DC},
there were no preceding indications of failure.

\subsubsection{Summary}
In summary, the memory leak was the only load usable for training a statistical
model to predict failure based only on reported errors.  As expected, the
software update did drastically reduce the effectiveness of a model trained
with failure data before the software update.  The boosted decision tree was
re-trainable after the software update whereas the \ac{SVM} was not.  This
suggests that both models should be used to ensure the \ac{AFP} framework
maintains at least one useful predictor and is adaptable to underlying system
changes.

Perhaps most interestingly, fault injection as was used in the original
\ac{AFP} framework implementation, had two extreme outcomes: 1) no failure, or
2) immediate failure.  In the controlled virtual environment, failure was
predictable using polled system health information, but perhaps the indicators
used to predict the failure were not actual errors but the fault injection tool
itself injecting faults.  Since during the golden runs, the fault injection
tool never wrote to another processes memory, it is possible that a predictor
could identify these operations if system health statistics are used as
features instead of reported errors.  Furthermore, even only using the Operator
for Missing Function Call (OMFC), there were still thousands of injection
points in the Windows Server 2008 operating system.  Identifying the handful
that may activate in a realistic way without crashing the target service
immediately is not trivial.  Clearly more work must be done to validate using
fault injection alone in the \ac{AFP} framework.

\hl{Ultimately, this work has shown that one minute of lead time is possible
under certain circumstances.  This amount of time is enough to perform a few
actions to mitigate failure.  First, the system could attempt to perform
software rejuvenation~\cite{candea2004microreboot}.  Failing that, it could
attempt to divert traffic to a redundant spare.}
