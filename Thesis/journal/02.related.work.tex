\section{Overview of \acrfull{OFP}} \label{chapter2}
\ac{OFP} is the act of evaluating a running system in real time to make a
prediction about whether a failure in a future state is
imminent~\citep{salfnerSurvey}.  Traditionally, failure is predicted using
statistical information about past failures offline before a system is fielded.
Unfortunately, given the complexity of modern computer systems and the infinite
number of ways in which they can be configured, this sort of offline analysis
is not helpful.

\citet{salfnerSurvey} published a survey paper that provides a comprehensive
summary of the state of the art on the topic of \ac{OFP}.  In addition to the
review of the literature up to the point of publication, they provide a summary
of definitions and measures of performance commonly used in the community for
couching the \ac{OFP} discussion.  The remainder of this section reviews those
definitions to build a foundation for the rest of this work.

\subsection{\acrfull{PFM}} \label{pfm}
\citet{salfnerSurvey} define \ac{PFM} as the process by which faults are
handled in a proactive way, analogous with \emph{fault tolerance} and
consisting of four steps: \ac{OFP}, diagnosis, action scheduling, and action
execution.  The final three stages of \ac{PFM} define how much lead time is
required to avoid a failure when predicted during \ac{OFP}.  \emph{Lead time}
is defined as the time between when failure is predicted and when that failure
will occur.  Lead time is one of the most critical elements of a failure
prediction approach.

\figonlinePrediction{0.8\textwidth}

Figure~\ref{fig:onlinePrediction} demonstrates the timeline associated with
\ac{OFP}.  The parameters used by the community to define a predictor are as
follows:
\begin{itemize}
	\item{Present Time: $t$}
  \item{Lead Time: $\Delta t_{l}$, is the total time at which a predictor makes
  an assessment about the current state.}
  \item{Data Window: $\Delta t_{d}$, represents the time window of data used
  for a predictor to make its assessment.}
  \item{Minimal Warning Time: $\Delta t_{w}$, is the amount of time required to
  avoid a failure if one is predicted.}
  \item{Prediction Period: $\Delta t_{p}$, is the time for which a prediction
  is valid.  As $\Delta t_{p} \rightarrow \infty$, the accuracy of the
  predictor approaches 100\% because every system will eventually fail.  As
  this happens, the usefulness of a predictor is diminished.}
\end{itemize}

\subsection{Faults, Errors, Symptoms, and Failures}
This research uses the definitions defined by \citet{avivzienis2004basic} as
interpreted and extended by \citet{salfnerSurvey} for the following terms:
failure; error (detected versus undetected); fault; and symptom.

\emph{Failure} is an event that occurs when the delivered service deviates from
correct service.  In other words, things can go wrong internally; as long as
the output of a system is what is expected, failure has not occurred.  An
\emph{error} is the part of the total state of the system that may lead to its
subsequent service failure.  \emph{Errors} are characterized as the point when
things go wrong.  Fault tolerant systems can handle errors without necessarily
evolving into failure.  There are two kinds of errors.  First, a \emph{detected
error} is an error that is reported to a logging service.  Second,
\emph{undetected errors} are errors that have not been identified by an error
detector.  Undetected errors are things like memory leaks.  Finally, a
\emph{fault} is the hypothesized root cause of an error.  Faults can remain
dormant for some time before manifesting themselves and causing an incorrect
system state.  In the memory leak example, the missing \emph{free} statement in
the source code would be the fault.  

\subsection{\acrfull{AFP} Framework} \label{afp}
The \ac{AFP} framework by \citet{irrera2015} presents a new approach to
maintaining the efficacy of failure predictors given underlying system changes.
The authors conducted a case study implementing the framework using
virtualization and fault injection on a web server.  

The framework generates failure data by injecting software faults using a tool
based on \ac{G-SWFIT}~\citep{gswfit} in a virtual environment for comparing and
automatically re-training predictors.  After implementing the \ac{AFP}
framework using a web server and an \ac{SVM} predictor, they report that their
findings demonstrate the framework is able to adapt to changes to an underlying
system which would normally render a predictor unusable.

In general, the use of simulated data is not well received by the community,
however \citet{irrera2010,irrera2014} report evidence supporting the claim that
simulated failure data is representative of real failure data.  Further, the
authors suggest that since systems are so frequently updated and failures are
in general rare events, real failure data is often not available.  Moreover,
the literature shows that even if there is a certain type of failure in
training data and a predictor can detect and predict that type of error
accurately, it will still miss failures not present in the training data.  By
injecting faults, there is an increased likelihood potential failure types are
represented in the training data.

\citet{irrera2015} reported good results and concluded that the \ac{AFP}
framework is an effective tool.  Unfortunately, the fault load used does not
completely represent all possible failures~\citep{kikuchi2014}.
