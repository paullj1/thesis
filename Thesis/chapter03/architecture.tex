\subsection{AFP Framework Implementation}
This experiment replicates the experiment in~\cite{irrera2015} except in place
of the web-server a Microsoft (MS) Windows Server running Active Directory (AD)
Domain Services.  In addition, several extensions to the original experiment
are made and presented here.  Multiple prediction techniques have been applied
using this framework to further generalize and validate the framework.  The
original AFP architecture is shown in Figure~\ref{fig:annotatedAFP} with the
parts that are modified in this work highlighted.  

\subsection{AFP Modules}
In~\cite{irrera2015}, the authors outline multiple modules into which they have
broken the AFP Framework for organizational purposes.  This research does not
modify these modules, instead, it takes a more granular approach and presents a
modified architecture and details each element of that architecture.

\figannotatedAFP  

The following sections detail the virtual environment in which this
architecture was constructed.  For reference, this virtual environment was
hosted on two VMWare ESXi 5.5 hypervisors each with two 2.6 GHz AMD Opteron
4180 (6 cores each) CPUs and 64 GB memory.  The individual virtual machines are
described in Tables~\ref{tab:hyp1}, and \ref{tab:hyp2}.

\tabHypervisorOne
\tabHypervisorTwo

\setcounter{secnumdepth}{5}

\subsection{Controller Hypervisor} \label{sec:controller} % 3.1.1
The controller functions in this experiment are split between two systems on a
single hypervisor seen on Table~\ref{tab:hyp2}.  One system is a Microsoft
Windows Server responsible for workload management and fault injection
management.  The additional Windows server also hosts remote desktop services
to allow the load generator to execute third party authentication with the
domain controller.  The other system is an Ubuntu 14.04 LTS server that
performs the failure prediction management and event management.  Each of
these functions is detailed in the following sections.

\subsubsection{Failure Prediction} \label{sec:failurePrediction} % 3.1.1.1
The failure prediction module predicts failure using machine learning
algorithms trained using the labelled training data generated by the rest of
this framework.  This module is constantly either training a new predictor
because a software update occurred, or predicting failure based on log messages
and other features produced by the production system.

THe AFP failure prediction function as outlined in~\cite{irrera2015}, is
performed by a support vector machine (SVM) predictor using \emph{libsvm}.
Additionally, the original experiment made use of a database that stored the
features and observations used for the failure prediction training algorithm.
This experiment does not modify the failure prediction module drastically as it
has already been shown in previous work that the online failure prediction area
of study is well explored~\cite{salfnerSurvey}.  This research makes use of a
different tool-set to execute the training and predicting phases.  Due to its
widespread use in the statistical community, the prediction and training
algorithms make use of the R programming language.

In this experiment, an SVM predictor is trained as done in~\cite{irrera2015},
as well as a... TODO maybe the method in~\cite{watanabe2014}?
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TO DO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%
% Need more details on actual prediction here.  Results of 623 paper will go  %
% here.                                                                       %
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%

\subsubsection{Fault Injection} \label{sec:faultInjectionMgr}
This module is responsible for managing the fault injector installed on the
clone of the production target system.  The purpose of the fault injector is to
force a loaded software application to fail in a realistic way so that the
indicators of that failure can be used to train the failure prediction
algorithm.

Irrera, et al.~\cite{irrera2015} use a tool implementing the G-SWFIT technique
for this module.  G-SWFIT was developed at the University of Coimbra in Coimbra
Portugal by Joao Duraes and Henrique Madeira~\cite{gswfit}.  The method is
widely implemented for use in software fault injection both commercially and
academically~\cite{natella2010,irrera2014,cotroneo2012,umadevi2015}.  Recently,
studies have questioned the representativeness of the failures generated by
G-SWFIT~\cite{kikuchi2014}.  In each case, the workload generated was critical
in creating representative faults.  This concern has been addressed in this
research and is discussed in Section~\ref{sec:workloadMgr}.

An additional concern has been that some faults that have been injected by use
of the G-SWFIT technique may not elude modern software testing and as a result
never actually occur in production software~\cite{natella2010}.  The
recommended remedy is to conduct source code analysis to determine which
pieces of code get executed most frequently and avoid fault injection in those
areas since they are most likely to be covered by unit tests.  Unfortunately,
the target is not an open source project and as a result, some of the faults
and resulting failures may never happen in a production environment.
Fortunately, the tool that has been developed for this research to do software
fault injection automatically scans each library loaded by the target
executable for fault injection points and then is capable of evenly
distributing the faults it does inject.

This work introduces an x86-64 implementation of the G-SWFIT technique called
W-SWFIT for Windows Software Fault Injection Tool.  The source code for W-SWFIT
has been published as open source on
Github\footnote{\url{https://github.com/paullj1/w-swfit/}} so that others may
use it for any of the reasons cited in the original G-SWFIT
paper~\cite{gswfit}.  For this research, the original plan was to use the same
tool used in~\cite{irrera2015} for fault injection.  Unfortunately, that tool
and all prior G-SWFIT implementations were incapable of injecting faults into
x86-64 binary executables.  Further, many of the commercial products that were
evaluated for this research were incapable of dealing with modern address space
layout randomization (ASLR).  As a result, W-SWFIT was developed for this
research and is capable of injecting faults into all user and kernel mode
applications on modern MS Windows operating systems.  

The key contributions of W-SWFIT are ASLR adaption and the x86-64 translation
we have performed.  G-SWFIT works by scanning binary libraries already in
memory for patterns (or operators) that match compiled errors in software
development.  The faults were based on the Orthogonal Defect
Classification~\cite{bridge1998} and can be seen in~\ref{tab:faults}.  As
pointed out in~\cite{salfnerSurvey} and~\cite{gswfit}, failures are ultimately
the result of software developer errors.  Unfortunately, much of the work done
in~\cite{gswfit} was on encoding common development errors as IA32 assembly
instructions so that working binary executable code could be mutated in memory
to introduce these errors in running applications.  The target application in
this research is strictly an x86-64 (also known as x64 or amd64) application
and the patterns identified in~\cite{gswfit} are incompatible.  Consequently, a
fault injection tool capable of mutating x86-64 instructions in the same way
was required.  W-SWFIT implements many of the operators in~\cite{gswfit} in
the x86-64 language by translating the operators seen in Table~\ref{tab:faults}
from IA32 to x86-64.  A simple example of this translation is shown on the
entry/exit points of a function in Tables~\ref{tab:translationThirtyTwo},
and~\ref{tab:translationSixtyFour}.  The rest of the translations can be seen
in source code for W-SWFIT.  In many cases, the translation was very simple,
but in others, the IA32 patterns did not cleanly map to x86-64 byte code.  When
this happened, great care was taken to ensure the pattern was correctly mapped
to x86-64.

\tabFaults
\tabTranslationThirtyTwo
\tabTranslationSixtyFour

\subsubsection{Workload Managmement} \label{sec:workloadMgr} 
The Workload module creates realistic work for the target system in the sandbox
hypervisor to do as a way of generating computational load.  Without this
module, it could take a very long time for an injected fault to manifest itself
as a failure.  Consider a missing \emph{free} statement and the consequent
memory leak.  A production target server may have a considerable amount of
memory and the leak could be very small.  To accelerate the possibility of
failure occurring, realistic load must be generated against the sandbox clone
of the production target.

In the original AFP case study, a Windows XP based web-server was used for a
target and therefore the load generation was done by a simple web request
generator.  As previously mentioned, realistic workload is critical in
generating realistic failure and consequently training a useful predictor.  As
a result, much emphasis is been placed on this module.  Since the target is
not a web server, it was not possible to use the same load generator as was
used in~\cite{irrera2015}.  Initial searches for a load generator suitable for
this research yeilded a tool developed by Microsoft that initiated remote
desktop connections to aid in sizing a terminal services
server\footnote{\url{http://www.microsoft.com/en-us/download/details.aspx?id=2218}}.
By executing a remote desktop session, the authentication and DNS functions of
the domain controller would also be loaded.  Unfortunately, this tool is no
longer maintained and would not execute on the target
machine\footnote{\url{https://social.technet.microsoft.com/Forums/windowsserver/en-US/2f8fa5cf-3714-4eb3-a895-c30e2b26862d/debug-assertion-failed-sockcorecpp-line-623}}.
Further searches for tools that would sufficiently load the domain controller
did not produce any results.  Consequently, a tool to produce realistic load
for a domain controller was developed for this research and is introduced here.

%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TO DO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%
% If D-PLG paper gets published, this should change                           %
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%

The Distributed PowerShell Load Generator (D-PLG) is a collection of Microsoft
PowerShell scripts designed to generate realistic traffic that will
sufficiently load a Microsoft domain controller.  Other network traffic
generators typically work by replaying traffic captured on a live network.
Unfortunately, due to the cryptographic nature of authentication, simply
replaying traffic will not load a service since the timestamps and challenge
responses will no longer be valid.  As a result, any replayed traffic will be
dropped and ignored by a live domain controller.  D-PLG solves this problem by
making native authentication requests by use of built-in PowerShell cmdlets
(command-lets).  By doing this, realistic authentication requests are sent to
the domain controller and are actually processed.  The functions performed by
the domain controller have been evaluated and D-PLG is designed to sufficiently
load each of the services responsible for performing those functions.
functions.

In this experiment, the DC is configured as it is in the Air Force Network
architecture.  After careful analysis, it has been determined that the major
roles in the Air Force architecture being performed are authentication and
domain name service (DNS).  Additionally, by use of native cmdlets, D-PLG is
capable of generating four kinds of traffic: web, mail, file sharing, and MS
remote desktop protocol (RDP).  D-PLG uses the MS Powershell environment to
generate the traffic in an effort to make the traffic as real as possible.
After building the tool, an experiment was constructed and executed on a scale
model of a production environment.  The scaled simulation network was built
using the recommendations of the Microsoft community for sizing a domain
controller~\cite{mak12} and tested by running the tool on five client machines
against the domain controller for five rounds of five minutes.  The results of
this test can be seen in Figures~\ref{fig:authDCPPS}, \ref{fig:authClientPPS},
\ref{fig:authDCMetrics}.

\figAuthDCPPS
\figAuthClientPPS
\figAuthDCMetrics

D-PLG makes use of client machines running a Windows operating system with
PowerShell version 4.0 or newer.  The controller asks each machine to generate
a configurable list of requests at evenly spaced intervals for a configurable
duration of time.  While this may not be realistic network traffic, it will
produce realistic load against a domain controller.  Since D-PLG depends on the
use of client machines, it is recommended that any load generation be conducted
during off-peak hours if spare client sized machines are not available.  It
should be noted however, that even with poorly resourced client machines (seen
in~\ref{tab:hyp1}), D-PLG was able to generate fifteen thousand authentication
sessions over a five minute period; approximately 10 authentication sessions
per machine, per second.  With modern workstations, the impact on these client
machines will likely be negligible and could likely be in use during load
generation.

Based on these results, and that a production domain controller should be at
approximately 40\% CPU utilization during peak utilization~\cite{mak12}, we
have concluded that D-PLG is capable of sufficiently loading the domain
controller over a sustained period of time for the purposes of implementing the
AFP framework and is used in this research.  Further, we have concluded that
D-PLG is capable of scaling to provide load against higher capacity domain
controllers by using only a few client machines.  There are many more uses for
a load generator of this type and we were not able to find a tool that is
capable of creating the same type of load so we have published these scripts on
Github\footnote{https://github.com/paullj1/AFP-DC/tree/master/D-PLG} for others
to use.

\subsubsection{Events Manager} \label{sec:eventsManagerMgr}
This module is responsible for receiving and managing log messages and other
events that may be used to train the failure prediction algorithm.  Irrera, et
al.~\cite{irrera2015} use the \emph{Logman} tool for event management in their
original case study.  Since the experimental environment was modelled after the
Air Force enterprise environment, the \emph{Solar Winds} log forwarding tool is
used to perform the functions in this module as it is already present on many
of the Air Force domain controllers.  The domain controllers on the Sandbox and
Target hypervisors forward all events to the Ubuntu virtual machine with the
\emph{rsyslog} server daemon configured to receive all messages.  These
messages are then processed and added to a SQL database for training and
prediction.  

\subsubsection{Sandbox Management} \label{sec:sandboxMgr} 
The purpose of the sandbox management module is to supervise the virtual
cloning of the production system that is made when a new predictor is to be
trained.  As Irrera, et al.~\cite{irrera2015,irrera2013} point out, it is
typically inappropriate to inject faults and cause failures in production
systems, so a virtual clone must be created for that purpose.

The sandbox is managed manually using Virtual Machine (VM) snapshots.  After an
initial stable state was configured, snapshots of every component of the
architecture were taken so that they could be reset after iterations of the
experiment.  It is important to note here that because VMWare has documented
APIs, in future work, this function could be automated.

\subsection{Sandbox Hypervisor} \label{sec:sandbox}
The sandbox is constructed on a single hypervisor implemented as seen on
Table~\ref{tab:hyp1}.  The following sections outline each module within this
module.

\subsubsection{Fault Injection} \label{sec:faultInjectionTool} 
This module is responsible for causing the target application to fail so that
labelled failure data can be generated in a short period of time.  As described
in Section~\ref{sec:faultInjectionMgr}, W-SWFIT has been developed to serve
this purpose and implements the G-SWFIT technique developed by Duraes, et
al.~\cite{gswfit} for fault injection.  The execution is controlled by the
Windows Server virtual machine on the Controller hypervisor through PowerShell
remote execution to reduce the interaction and potential to introduce bias into
the training data.  The tool allows us to inject a comprehensive list of faults
into the AD Services processes and binary libraries which are mostly contained
within the `lsass.exe' process.  Since many of the critical functions performed
by the AD Services processes are performed in one library called `ntdsa.dll',
it is the focus of fault injection.

This function is extended by this research to include failure as a result of
excessive load and failure as a result of a corrupt database.
Section~\ref{sec:extensions} covers these extensions in more depth.

\subsubsection{Monitoring} \label{sec:sandboxMonitoringTool} 
The purpose of this module is to capture some evidence or indication of pending
failure so that it may be used to train a prediction algorithm.  Irrera, et
al.~\cite{irrera2015} use the \emph{Logman} tool in their original study but
because the experimental infrastructure used in this research is modelled after
production Air Force networks, the \emph{Solar Winds} log forwarding tool is
used because it is already present in the Air Force architecture.  The tool is
a lightweight application that simply forwards windows events to a syslog
server.

\subsubsection{Sandbox Workload}  \label{sec:sandboxWorkload} 
The sandbox workload module is likely the most critical module in the entire
framework.  Its purpose is to create realistic work for the target application
to do before faults are injected.  If the workload is not realistic, then the
failures that occur after fault injection will not be representative of real
failures and any data or indicators collected cannot be used to train an
effective prediction algorithm.

Irrera, et al.~\cite{irrera2015} used a web traffic generator called TPC-W
installed on a single machine in their original study because their target was
a web server.  Because the domain controller does not respond to web-requests
and a tool had not previously been written for this application, a tool was
developed for this research called D-PLG. D-PLG is a tool that generates
approximately ten full-stack authentication sessions requests per second in
order to sufficiently load the domain controller.  D-PLG is a distributed tool
and requires the use of client machines as a result.  This module is
represented by those client machines.  In this experiment, the client portion
of D-PLG is installed on five client machines managed by the central workload
manager as discussed in Section~\ref{sec:workloadMgr}.

\subsection{Target Hypervisor} \label{sec:target}
The target hypervisor was constructed as a clone of the sandbox hypervisor seen
on Table~\ref{tab:hyp1}.  The following section outlines the monitoring tool
installed on the domain controller on this hypervisor.  It should be noted here
that while the client machines were cloned as well for convenience, they were
not used in this experiment.

\subsubsection{Monitoring} \label{sec:targetMonitoringTool}
The monitoring module is exactly the same as the sandbox monitoring module and
for this experiment, the \emph{Solar Winds} syslog forwarding tool is used.  To
ensure that the messages that are sent are uniquely identified by the
controller, the hostname of the target machine must be different from the
hostname of the sandbox target machine.

\setcounter{secnumdepth}{1}

