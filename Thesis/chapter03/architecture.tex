\subsection{Overview}

\subsection{AFP Framework Extension}
Our experiment replicates the experiment in~\cite{irrera2015} except in place
of the web-server a Microsoft (MS) Windows Server running Active Directory (AD)
Domain Services.  We have applied multiple prediction techniques using this
framework to further generalize and validate the framework.  The original AFP
architecture can be seen in Figure~\ref{fig:annotatedAFP} with the parts that
would need to be modified for our experiment highlighted and numbered 8,9, and
12.  The rest of this section is organized to detail each element of
Figure~\ref{fig:annotatedAFP} by the associated number.

\subsection{Components of the Experiment}
In~\cite{irrera2015}, the authors outline multiple modules into which they have
broken the AFP Framework for organizational purposes.  Our work does not modify
these modules, instead, we have taken a more granular approach and present a
modified architecture and detail each element of that architecture.

\figannotatedAFP  

The following sections will reference the virtual environment in which this
architecture was constructed.  For reference, this virtual environment was
hosted on two VMWare ESXi 5.5 hypervisors each with two 2.6 GHz AMD Opteron
4180 (6 cores each) CPUs and 64 GB memory.  The individual virtual machines are
detailed in Tables~\ref{tab:hyp1}, and \ref{tab:hyp2}.

\tabHypervisorOne
\tabHypervisorTwo

\setcounter{secnumdepth}{5}

\subsection{Controller Hypervisor} \label{sec:controller} % 3.1.1
The controller functions in our experiment are split between two systems on a
single hypervisor seen on Table~\ref{tab:hyp2}.  One system is a Microsoft
Windows Server responsible for workload management and fault injection
management.  The additional Windows server also hosts remote desktop services
to allow our load generator to execute third party authentication with the
domain controller.  The other system is an Ubuntu 14.04 LTS server that
performs the failure prediction management and event management.  Each of
these functions is detailed in the following sections.

\subsubsection{Failure Prediction} \label{sec:failurePrediction} % 3.1.1.1
The purpose of this module is to actually predict failure using machine
learning algorithms that are trained using the labelled training data generated
by the rest of this framework.  This module is constantly either training
a new predictor because a software update occurred, or predicting failure based
on log messages and other features produced by the production system.

In~\cite{irrera2015}, the failure prediction management function is performed
by an SVM predictor using libsvm.  Additionally, the original experiment made
use of a database that stored the features used for the failure prediction
training algorithm.  Our experiment does not modify this module drastically
as it has already been shown~(\cite{salfnerSurvey}) that we are not
experiencing a shortage in the number of available quality predictors.  We have
however chosen a different tool-set to execute the training and predicting
phases.  Due to its widespread use in the statistical community, our prediction
and training algorithms made use of the R programming language.

In our experiment, we train an SVM predictor as done in~\cite{irrera2015}, as
well as a... TODO maybe the method in~\cite{watanabe2014}?
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TO DO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%
% Need more details on actual prediction here.  Results of 623 paper will go  %
% here.                                                                       %
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%

\subsubsection{Fault Injection} \label{sec:faultInjectionMgr}
This module is responsible for managing the fault injector installed on the
clone of the production target system.  The purpose of the fault injector is to
force a loaded software application to fail in a realistic way so that the
indicators of that failure can be used to train the failure prediction
algorithm.

Irrera et al. use a tool implementing the G-SWFIT technique for this module
in~\cite{irrera2015}.  G-SWFIT was developed at the University of Coimbra in
Coimbra Portugal by Joao Duraes and Henrique Madeira~\cite{gswfit}.  The method
is widely implemented for use in software fault injection both commercially and
academically~\cite{natella2010,irrera2014,cotroneo2012,umadevi2015}.  Recently,
studies have questioned the representativeness of the failures generated by
G-SWFIT~\cite{kikuchi2014}.  In each case, the workload generated was critical
in creating representative faults.  We address this in
Section~\ref{sec:workloadMgr}.

An additional concern has been that some faults that have been injected by use
of the G-SWFIT technique may not elude modern software testing and as a result
never actually occur in production software~\cite{natella2010}.  The
recommended remedy is to conduct source code analysis to determine which
pieces of code get executed most frequently and avoid fault injection in those
areas since they are most likely to be covered by unit tests.  Unfortunately,
our target is not an open source project and as a result, some of our faults
and resulting failures may never happen in a production environment.
Fortunately, the tool that we have developed to do software fault injection
automatically scans each library loaded by the target executable for fault
injection points and then is capable of evenly distributing the faults it does
inject.

For our experiment, the original plan was to use the same tool used
in~\cite{irrera2015} for fault injection.  Unfortunately, that tool and all
prior implementations of the G-SWFIT were incapable of injecting faults into
x86-64 binary executables.  Further, even many of the commercial products that
we evaluated were incapable of dealing with modern address space layout
randomization (ASLR).  As a result, we have developed a tool capable of
injecting faults into all user and kernel mode applications on modern MS
Windows operating systems.  In this work we introduce an x86-64 implementation
of the G-SWFIT technique that we call W-SWFIT for Windows Software Fault
Injection Tool.  We have published the source code as open source on
Github\footnote{\url{https://github.com/paullj1/w-swfit/}} and hope that others
may find it useful for many of the reasons cited in the original G-SWFIT
paper~\cite{gswfit}.

The key contributions of W-SWFIT are ASLR adaption and the x86-64 translation
we have performed.  G-SWFIT works by scanning binary libraries already in
memory for patterns (or operators) that match compiled errors in software
development.  The faults were based on the Orthogonal Defect
Classification~\cite{bridge1998} and can be seen in~\ref{tab:faults}.  As
pointed out in~\cite{salfnerSurvey} and~\cite{gswfit}, failures are ultimately
the result of software developer errors.  Unfortunately, much of the work done
in~\cite{gswfit} was on encoding common development errors as IA32 assembly
instructions so that working binary executable code could be mutated in memory
to introduce these errors in running applications.  Our target is strictly an
x86-64 (also known as x64 or amd64) application and the patterns identified
in~\cite{gswfit} are incompatible.  Consequently, we needed our tool to mutate
x86-64 instructions in the same way.  We have implemented each of the operators
in~\cite{gswfit} in the x86-64 language by translating the operators seen
in~\ref{tab:faults} from IA32 to x86-64.  A simple example of this translation
can be seen on the entry/exit points of a function in
Tables~\ref{tab:translationThirtyTwo}, and~\ref{tab:translationSixtyFour}.  The
rest of the translations can be seen in the source code.  In many cases, the
translation was very simple, but in others, the IA32 patterns did not cleanly
map to x86-64 byte code.  When this happened, great care was taken to ensure
the pattern was correctly mapped to x86-64.

\tabFaults
\tabTranslationThirtyTwo
\tabTranslationSixtyFour

\subsubsection{Workload} \label{sec:workloadMgr} 
The Workload module is responsible for orchestrating the load against the
target system in the sandbox hypervisor.  Without this module, it could take a
very long time for an injected fault to manifest itself as a failure.  Consider
a missing \emph{free} statement and the consequent memory leak.  A production
target server may have a considerable amount of memory and the leak could be
very small.  To accelerate the possibility of failure occurring, realistic load
must be generated against the sandbox clone of the production target.

In the original AFP case study, a Windows XP based web-server was used for a
target and therefore the load generation was done by a simple web request
generator.  As previously mentioned, realistic workload is critical in
generating realistic failure and consequently training a useful predictor.  As
a result, we have placed much emphasis on this module.  Since our target is not
a web server, we could not use the same load generator used
in~\cite{irrera2015}.  Our initial search for a load generator yielded a tool
developed by Microsoft that initiated remote desktop connections to aid in
sizing a terminal services
server\footnote{\url{http://www.microsoft.com/en-us/download/details.aspx?id=2218}}.
By executing a remote desktop session, the authentication and DNS functions of
the domain controller would also be loaded.  Unfortunately, this tool is no
longer maintained and would not execute on our target
machine\footnote{\url{https://social.technet.microsoft.com/Forums/windowsserver/en-US/2f8fa5cf-3714-4eb3-a895-c30e2b26862d/debug-assertion-failed-sockcorecpp-line-623}}.
Further searches for tools that would sufficiently load our domain controller
did not produce any results.  Consequently, we began developing our own tool
and introduce it here. 
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TO DO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%
% If D-PLG paper gets published, this should change                           %
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%

The Distributed PowerShell Load Generator (D-PLG) is a collection of Microsoft
PowerShell scripts designed to generate realistic traffic that will
sufficiently load a Microsoft domain controller.  Other network traffic
generators typically work by replaying traffic captured on a live network.
Unfortunately, due to the cryptographic nature of authentication, simply
replaying traffic will not load a service since the timestamps and challenge
responses will no longer be valid.  As a result, any replayed traffic will be
dropped and ignored by a live domain controller.  D-PLG solves this problem by
making native authentication requests by use of built-in PowerShell cmdlets
(command-lets).  By doing this, we can guarantee that realistic authentication
requests are sent to our domain controller and are actually processed.  We have
evaluated the functions our domain controller performs and have built D-PLG to
be able to sufficiently load each of the services responsible for performing
those functions.

For our purposes, we considered the DC as employed in the Air Force Network
architecture.  After careful analysis, we have determined that the major roles
in that architecture being performed are authentication and domain name service
(DNS).  Additionally, by use of native cmdlets, D-PLG is capable of generating
four kinds of traffic: web, mail, file sharing, and MS remote desktop protocol
(RDP).  We use MS's Powershell environment to generate the traffic in an effort
to make the traffic as real as possible.  After building the tool we
constructed and tested it on a scale model of a production environment.  The
scaled simulation network was built using the recommendations of the Microsoft
community for sizing a domain controller~\cite{mak12} and tested by running the
tool on five client machines against our domain controller for five rounds of
five minutes.  The results of this test can be seen in
Figures~\ref{fig:authDCPPS}, \ref{fig:authClientPPS}, \ref{fig:authDCMetrics}.

\figAuthDCPPS
\figAuthClientPPS
\figAuthDCMetrics

D-PLG makes use of client machines running a Windows operating system with
PowerShell version 4.0 or newer.  The controller asks each machine to generate
a configurable list of requests at evenly spaced intervals for a configurable
duration of time.  While this may not be realistic network traffic, it will
produce realistic load against a domain controller.  Since D-PLG depends on the
use of client machines, we recommend conducting any load generation during
off-peak hours if spare client sized machines are not available.  It should be
noted however, that even with our poorly resourced client machines (seen
in~\ref{tab:hyp1}), we were able to generate fifteen thousand authentication
sessions over a five minute period; approximately 10 authentication sessions
per machine, per second.  With modern workstations, the impact on these client
machines will likely be negligible and could likely be in use during load
generation.

Based on these results, and that a production domain controller should be at
approximately 40\% CPU utilization during peak utilization~\cite{mak12}, we
have concluded that D-PLG is capable of sufficiently loading our domain
controller over a sustained period of time for the purposes of implementing the
AFP framework and is used in this research.  Further, we have concluded that
D-PLG is capable of scaling to provide load against higher capacity domain
controllers by using only a few client machines.  There are many more uses for
a load generator of this type and we were not able to find a tool that is
capable of creating the same type of load so we have published these scripts on
Github\footnote{https://github.com/paullj1/AFP-DC/tree/master/D-PLG} for others
to use.

\subsubsection{Events Manager} \label{sec:eventsManagerMgr}
This module is responsible for receiving and managing log messages and other
events that may be used to train the failure prediction algorithm.  Irrera et
al. use the \emph{Logman} tool for event management in their original case
study in~\cite{irrera2015}.  Since we have modelled our environment after the
Air Force enterprise environment, we have chosen the \emph{Solar Winds} log
forwarding tool to perform the functions in this module as it is already
present on many of the Air Force domain controllers.  The domain controllers on
the Sandbox and Target hypervisors forward all events to the Ubuntu virtual
machine with the \emph{rsyslog} server daemon configured to receive all
messages.  These messages are then processed and added to a SQL database for
training and prediction.  

\subsubsection{Sandbox Management} \label{sec:sandboxMgr} 
The purpose of the sandbox management module is to orchestrate the virtual
clone of the production system that is made when a new predictor is to be
trained.  As Irrera et al. point out, we cannot reasonably expect to be able to
inject faults and cause failures in production systems, so a virtual clone must
be created for that purpose.

Our sandbox is managed manually using Virtual Machine (VM) snapshots.  After an
initial stable state was configured, snapshots of every component of the
architecture were taken so that they could be reset after iterations of the
experiment.  It is important to note here that because VMWare has documented
APIs, in future work, this function could be automated.

\subsection{Sandbox Hypervisor} \label{sec:sandbox}
The sandbox has been constructed on a single hypervisor implemented as seen on
Table~\ref{tab:hyp1}.  The following sections outline each module within this
module.

\subsubsection{Fault Injection} \label{sec:faultInjectionTool} 
This module is responsible for causing the target application to fail so that
labelled failure data can be generated in a short period of time.  As described
in Section~\ref{sec:faultInjectionMgr}, we have developed a tool that
implements the G-SWFIT technique developed by Duraes et al.  in~\cite{gswfit}
for fault injection.  The execution is controlled by our Windows Server virtual
machine on the Controller hypervisor through PowerShell remote execution to
reduce our interaction and potential to introduce bias into the training data.
The tool allows us to inject a comprehensive list of faults into the AD
Services processes and binary libraries which are mostly contained within the
`lsass.exe' process.  Since many of the critical functions performed by the AD
Services processes are performed in one library called `ntdsa.dll', it is the
focus of our fault injection.  We also focus attention in generating the
training data 

\subsubsection{Monitoring} \label{sec:sandboxMonitoringTool} 
The purpose of this module is to capture some evidence or indication of pending
failure so that it may be used to train a prediction algorithm.  Irrera et al.
use the \emph{Logman} tool in their original study but because we are modelling
our architecture after production Air Force networks, we have chose the
\emph{Solar Winds} log forwarding tool.  The tool is a lightweight application
that simply forwards windows events to a syslog server.

\subsubsection{Workload}  \label{sec:sandboxWorkload} 
The workload module is likely the most critical module in the entire framework.
Its purpose is to create realistic work for the target application to do before
faults are injected.  If this workload is not realistic, then the failures that
occur after fault injection will not be representative of real failures and any
data or indicators collected cannot be used to train an effective prediction
algorithm.

Irrera et al. used a web traffic generator called TPC-W in their original study
because their target was a web server.  Because our service does not respond to
web-requests and a tool had not previously been written for this application,
we have designed D-PLG, a tool that generates approximately ten full-stack
authentication sessions requests per second in order to sufficiently load the
domain controller.  In our experiment, this tool is installed on five client
machines which make it capable of sufficiently loading the domain controller
that we have built for this experiment.

\subsection{Target Hypervisor} \label{sec:target}
The target hypervisor was constructed as a clone of the sandbox hypervisor seen
on Table~\ref{tab:hyp1}.  The following section outlines the monitoring tool
installed on the domain controller on this hypervisor.  It should be noted here
that while we did clone the client machines for convenience, they were not used
in our experiments.

\subsubsection{Monitoring} \label{sec:targetMonitoringTool}
This module is exactly the same as the sandbox monitoring module and for our
experiment, we have used the \emph{Solar Winds} syslog forwarding tool.  To
ensure that the messages that are sent are uniquely identified by the
controller, the hostname of the target machine must be different from the
hostname of the sandbox target machine.

\setcounter{secnumdepth}{1}

