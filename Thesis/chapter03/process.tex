This work extends the Adaptive Failure Prediction framework~\cite{irrera2015}
by conducting another case study with a Microsoft Windows Server acting as an
active directory service with a new implementation of the G-SWFIT technique for
the x86-64 architecture as well as a more representative fault load.  The case
study is then taken a step further by showing that the approach holds for other
predictors (maybe).  Specifically, findings are reported after implementing
this framework with the predictor used by Irrera, et al.~\cite{irrera2015} in
their case study and the predictor used by Watanabe, et
al.~\cite{watanabe2014}.

This section outlines the step-by-step procedure by which the extended AFP is
evaluated to show how effective it is when used on Windows Server deployments.
This is done by dividing the steps taken in an experiment into the three major
phases as defined in~\cite{irrera2015}: preparation phase, execution phase, and
training phase.

\subsection{Preparation Phase}
In this phase the AFP is prepared to run for the first time as described
in~\cite{irrera2015}.  The Cross Industry Standard Process for Data Mining
(CRISP-DM)~\cite{crispdm} should be applied to this situation when evaluating
how to best apply the AFP for a particular target.  For the purposes of this
research, our focus is on the Microsoft Windows Directory Services and
predicting failure in those services.  To demonstrate the efficacy of the AFP,
a predictor must be evaluated before and after a significant software update.
As a result, the most critical preparation made in evaluating this framework is
to hold back all software updates on the target system prior to the first run
of the execution phase.  

This phase is ultimately the implementation of the framework.  Each module of
the implementation for this work is detailed in
Section~\ref{sec:implementation} and is therefore not discussed further here.  

\subsection{Execution Phase}
A general outline of this process can be seen in
Figure~\ref{fig:ExecutionPhase}.  This phase is divided into three major
states: data collection and failure prediction, event checking, and
training/update as described in this section.

\figExecutionPhase

\subsubsection{Data Collection and Failure Prediction}
In this phase, the system has a working predictor providing input to some sort
of decision system.  It should be noted here that this decision system does not
have to be automated.  The system in this phase is making failure predictions
about the current state based on the last run of the training phase.  For the
Air Force application, an operator will be the decision maker and the output of
the AFP will be a message to that operator.  

\subsubsection{Event Checking}
Concurrent with the data collection and failure prediction sub-phase, the AFP
continuously monitors events that may alter the underlying system.  For this
experiment, these events are software updates.  The output of each episode of
this phase is a binary decision to either begin the training phase, or not.  In
this experiment, the training phase is manually triggered upon completion of a
major software update.

\subsubsection{Failure Predictor (Re-)Training and Update}
The purpose of this sub-phase is to initiate the training phase and compare its
results (a new predictor) with the currently employed predictor.  Should the
new predictor perform better, the old predictor is replaced by the new.

\subsection{Training Phase}
The training phase is broken down into five major steps:  target replication,
data generation \& collection, dataset building, predictor training, and
analysis.  The general flow can be seen in Figure~\ref{fig:TrainingPhase}.
Each phase is outlined in the following sub-sections.

\figTrainingPhase

\subsubsection{Target Replication}
During this phase a virtual clone of the target is made.  After the clone is
made, the fault injection and monitoring software must be installed.  In this
experiment, the monitoring tool is the same as the production system but care
must be taken to ensure the host-name is changed so the log messages generated
during this phase are not confused with messages from the production system.

\subsubsection{Data Generation \& Collection}
The purpose of this phase is to generate the data to train a new prediction
algorithm.  As a result, this sub-phase must be executed several times to
generate statistically meaningful datasets.  In this phase, the controller
triggers the cloned target startup.  Once startup is complete and the system
enters an idle state, the monitoring tool begins collecting data from the
target.  After monitoring has begun, the workload is started.  Once the
workload has entered a steady state, the fault load is started.  Finally, when
failure occurs, monitoring stops, the workload stops, and the system is
rebooted for the next run.  To generate golden data, the first run omits the
fault injection step.

The most critical part of this process is labelling the data when failure
occurs.  For the purposes of this experiment, D-PLG reports when authentication
fails and transmits a syslog message to the controller.  For this research,
failure has been defined by two criteria, the first is when authentication with
known good credentials fails.  The second is classified by when authentication
with known good credentials takes longer than thirty seconds (and it can then
be assumed that the service has crashed).

\subsubsection{Dataset Building}
In this phase, the raw syslog messages are formatted and encoded to train the
predictor.  The messages generated by the load generator are removed and in
each case the neighboring message with the closest timestamp is labelled as
when the failure occurred.

Irrera, et al.~\cite{irrera2015} loaded all event messages into a database for
processing.  In this work, the events are initially stored in a flat file on
the Ubuntu machine by the syslog daemon.  During this phase, a tool is run that
divides each of the $n$ event messages into $n$ observations with the following
three features: failure, timestamp, and message.  The failure feature is a
binary bit set if the event occurred within a configurable time window $\delta
t$ before the actual failure as defined by Salfner, et
al.~\cite{salfnerSurvey}.  The messages are processed by assigning a unique
integer for each unique word in the log messages.  The integers are then
concatenated resulting in a single feature (TODO: Better process?).  The
features for each observation are then combined in an $n \times 3$ matrix for
training.

\subsubsection{Predictor Training}
The purpose of this phase is to use the data generated by the forced failure of
the virtual clone to train a machine learning algorithm to classify a system as
failure prone or not.  

During this phase, each of the $k$ datasets produced by the $k$ runs of the
execution phase, each containing a single failure, are used to train a support
vector machine.  Each dataset is an $n \times 3$ matrix where $n$ is the
number of events recorded in the syslog for each run of the execution phase.
These $k$ datasets are used to conduct a $k$-fold cross validation training
and evaluation process where the first $k - 1$ datasets are used to train a
support vector machine.  The remaining set is used to validate the trained
model.  The data is then rotated and repeated $k$ times.

At each phase, the number of events that are correctly classified as failure
prone and occurring within the specified time window $\delta t$ are recorded as
well as the number of events incorrectly classified as failure prone occurring
within the time window.  These figures are then used to calculate the measures
of performance in the \emph{Analysis} phase.

\subsubsection{Analysis}
During this phase, the precision, recall, and area under the ROC curve are
computed using the figures measured in the previous phase so that the new
predictor can be compared against the old.  In this experiment, the focus is on
the precision and recall of the algorithm.  Since failure is such a rare event,
accuracy is not a very meaningful measure of performance and thus is not used.
