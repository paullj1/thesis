This work generalizes the Adaptive Failure Prediction framework outlined
in~\cite{irrera2015} by conducting another case study with a Microsoft Windows
Server acting as an active directory service.  The case study is then taken a
step further by showing that the approach holds for other predictors (maybe).
Specifically, findings are reported after implementing this framework with
the predictor used by Irrera et al. in their case study and the predictor used
by Watanabe et al. here~\cite{watanabe2014}.

This section outlines the step-by-step procedure by which the extended AFP is
evaluated to show how effective it is when used on Windows Server deployments.
This is done by dividing the steps taken in an experiment into the three major
phases as defined in~\cite{irrera2015}: preparation phase, execution phase, and
training phase.

\subsection{Preparation Phase}
In this phase the AFP is prepared to run for the first time as described
in~\cite{irrera2015}.  For the purposes of this research, we have mostly
described the steps that we have taken for this phase in the previous section.
The point of this phase is to prepare the first predictor on the production
system and implement the modules of the AFP for future predictor training.  The
most critical preparation we have made in evaluating this framework is to hold
back all software updates on our target system prior to the first run of the
execution phase.

\subsection{Execution Phase}
A general outline of this process can be seen in
Figure~\ref{fig:ExecutionPhase}.  This phase is divided into three major states
data collection and failure prediction, event checking, and training/update as
described in this section.

\figExecutionPhase

\subsubsection{Data Collection and Failure Prediction}
In this phase, the system has a working predictor providing input to some sort
of decision system.  It should be noted here that this decision system does not
have to be automated.  The system in this phase is making failure predictions
about the current state based on the last run of the training phase.  For the
Air Force application, an operator will likely be the decision maker and the
output of the AFP will be a message to that operator.  

\subsubsection{Event Checking}
Concurrent with the data collection and failure prediction sub-phase, the AFP
will be continuously monitoring events that may alter the underlying system.
In our experiment, these events are software updates.  The output of each
episode of this phase is a binary decision to either begin the training phase,
or not.  In our experiments, we manually trigger the training phase upon
completion of a major software update.

\subsubsection{Failure Predictor (Re-)Training and Update}
The purpose of this sub-phase is to initiate the training phase and compare its
results (a new predictor) with the currently employed predictor.  Should the
new predictor perform better, the old predictor will be replaced by the new.

\subsection{Training Phase}
The training phase is broken down into five major steps:  target replication,
data generation \& collection, datatset building, predictor training, and
analysis.  The general flow can be seen in Figure~\ref{fig:TrainingPhase}.
Each phase is outlined in the following sub-sections.

\figTrainingPhase

\subsubsection{Target Replication}
During this phase a virtual clone of the target is made.  After the clone is
made, the fault injection and monitoring software must be installed.  In our
experiments, the monintoring tool is the same as the production system but care
must be taken to ensure the hostname is changed so the log messages generated
during this phase are not confused with messages from the production system.

\subsubsection{Data Generation \& Collection}
The purpose of this phase is to generate the data to train a new prediction
algorithm.  As a result, this sub-phase must be executed several times to
generate statistically acceptable datasets.  In this phase, the controller will
trigger the cloned target to startup.  Once startup is complete and the system
enters an idle state, the monitoring tool begins collecting data from the
target.  After monitoring has begun, the workload is started.  Once the
workload has entered a steady state, the fault load is started.  Finally, when
failure occurs, monitoring stops, the workload stops, and the system is
rebooted for the next run.  To generate golden data, the first run omits the
fault injection step.

The most critical part of this process is labelling the data when failure
occurs.  For the purposes of our experiment, D-PLG reports when authentication
fails and transmits a syslog message to our controller.  We have established
two criteria for failure, the first is when authentication with known good
credentials fails.  The second is classified by when authentication with known
good credentials takes longer than thirty seconds (and we can then assume that
the service has crashed).

\subsubsection{Dataset Building}
In this phase, the raw syslog messages are formatted and encoded to train the
predictor.  The messages generated by our load generator are removed and in
each case the neighboring message with the closest timestamp is labelled as
when the failure occured.

TODO:  Describe process for encoding messages for training.
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TO DO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%
% Describe process for encoding messages for training.                        %
%%TODO%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%TODO%%

\subsubsection{Predictor Training}
The purpose of this phase is to use the data generated by the forced failure of
the virtual clone to train a machine learning algorithm to classify a system as
failure prone or not.  This phase is broken into three sub-steps.  First, the
prediction algorithm is trained using a subset of the generated data.  The
held-out data from that set is then used to test the prediction algorithm.
Finally, the output of the algorithm is collected for analysis and measure of
performance calculation in the next step.

\subsubsection{Analysis}
During this phase, the measures of performance in Chapter~\ref{chapter2} are
calculated so that the new predictor can be compared against the old.  In our
experiment, we focused on the precision and recall of the algorithm.  Since
failure is such a rare event, accuracy isn't a very meaningful measure of
performance.

